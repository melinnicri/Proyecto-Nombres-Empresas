# Prueba T√©cnica: Scraping de Informaci√≥n en P√°gina Web

**Extracci√≥n automatizada de datos de contacto empresarial desde fuentes p√∫blicas**  
**Autora:** Amelia Cristina Herrera Brice√±o  
**Rol:** Data Analyst & Scientist en transici√≥n hacia BI y automatizaci√≥n  
**Entorno de trabajo:** Python ¬∑ Pandas ¬∑ BeautifulSoup ¬∑ Regex ¬∑ CSV ¬∑ Exploraci√≥n reproducible  
**Fecha de entrega:** 15 Agosto 2025  

<p align="center"><img src="https://github.com/melinnicri/Proyecto-Nombres-Empresas/blob/main/image/scrapeo.png"></p>

---

## 1. Problema

Se cuenta con listados de empresas (nombre y CIF), pero se carece de sus datos de contacto directo 
(emails corporativos, tel√©fonos, direcciones). Para establecer comunicaci√≥n comercial, es necesario 
localizar y extraer autom√°ticamente esta informaci√≥n desde sus sitios web oficiales u otras fuentes 
p√∫blicas disponibles en internet.

---

## 2. Soluci√≥n T√©cnica

Desarrollar un proceso de scraping web que permita obtener datos de contacto empresariales de forma 
automatizada a partir de los nombres de las empresas.

---

## 3. Alcance de la Prueba

- **Dataset:** Muestra de 50‚Äì100 empresas espa√±olas (nombre + CIF)
- **Proceso esperado:**
  1. Localizar la web corporativa oficial de cada empresa
  2. Extraer datos de contacto mediante scraping web
- **Datos objetivo:**
  - URL oficial
  - Email corporativo principal (info@, contacto@, comercial@)
  - Tel√©fono de contacto
  - Direcci√≥n postal completa
- **Entregables:**
  - Archivo CSV/Excel con los datos obtenidos
  - Documentaci√≥n t√©cnica del proceso
  - Registro de limitaciones (robots.txt, captchas, etc.)
  - Tiempo estimado de procesamiento por empresa
  - Coste estimado del proceso
  - Video explicativo del flujo t√©cnico

---

## 4. Implementaci√≥n T√©cnica

- Scraping sincr√≥nico reproducible: `requests` + `BeautifulSoup` + `regex`
- Limpieza post-scraping: DataFrames exportables, validaci√≥n de encoding y estructura
- Exportaci√≥n: Archivos CSV listos para an√°lisis y visualizaci√≥n

---

## 5. M√©tricas y Evaluaci√≥n

- **Tasa de √©xito:** porcentaje de empresas con datos completos
- **Limitaciones encontradas:** robots.txt, captchas, p√°ginas sin datos visibles
- **Tiempo estimado por empresa:** medido con barra de progreso
- **Coste estimado:** bajo en entorno local; escalable con APIs

---

## 6. Video Explicativo

üé• Se graba un video mostrando el flujo, explicando el c√≥digo y los resultados obtenidos.  
_Pendiente de edici√≥n y subida._

---

## 7. Escalabilidad

Para flujos masivos, se propone una versi√≥n asincr√≥nica con `aiohttp` o `crawl4ai`, que mejora el 
rendimiento y permite scraping concurrente.

---

## 8. Comparativa de M√©todos de B√∫squeda de URLs Oficiales


| M√©todo                    | Gratuito | L√≠mite         | Costo estimado | Ventajas                            | Desventajas                           |
|---------------------------|----------|----------------|----------------|-------------------------------------|---------------------------------------|
| googlesearch (scraping)   | S√≠       | No oficial     | $0             | Sin registro, ideal para pruebas    | Bloqueos, resultados no estructurados |
| SerpAPI                   | S√≠ (250) | Pago desde $50 | ~$50           | JSON limpio, sin bloqueo            | Requiere API Key, coste mensual       |
| Zenserp                   | S√≠ (50)  | Pago desde $29 | ~$29           | F√°cil de usar, resultados precisos  | Menor volumen gratuito                |
| Google Custom Search API  | S√≠ (100) | $5/1000        | ~$5            | Oficial, configurable               | Requiere configuraci√≥n previa         |
| Bing Search API           | S√≠ (1000)| Pago desde $3  | ~$3            | Econ√≥mica, buena cobertura          | Menor precisi√≥n en empresas locales   |

---

## 9. Recomendaci√≥n Final

Para esta prueba t√©cnica (50‚Äì100 empresas), se recomienda utilizar `googlesearch` por su simplicidad 
y coste cero. Para escalar el proceso, se sugiere evaluar APIs comerciales como `SerpAPI` y adoptar 
scraping asincr√≥nico con `aiohttp` o `crawl4ai`.

---

## 10. Correcci√≥n Fon√©tica y Sem√°ntica de Nombres Empresariales

Objetivo: corregir nombres empresariales con validaci√≥n fon√©tica (`rapidfuzz`) y sem√°ntica (`unidecode`), 
asegurando trazabilidad y limpieza formal.

### Ejemplo de correcci√≥n fon√©tica

```python
import re
from unidecode import unidecode

def normalizar_nombre(nombre):
    nombre = unidecode(nombre)
    nombre = re.sub(r'[^\w\s]', '', nombre)
    nombre = nombre.strip().upper()
    return nombre

nombre_1 = "Compa√±√≠a ABC S.A."
nombre_2 = "Compania - ABC, SA"

print(normalizar_nombre(nombre_1))  # COMPANIA ABC SA
print(normalizar_nombre(nombre_2))  # COMPANIA ABC SA
```

---

## 11. Flujo de Correcci√≥n y Validaci√≥n

Este m√≥dulo se encarga de limpiar, corregir y validar los nombres empresariales antes del scraping,
asegurando trazabilidad y coherencia sem√°ntica.

| Archivo           | Prop√≥sito principal                                                         |
|-------------------|-----------------------------------------------------------------------------|
| `normalizacion.py`| Funciones para limpiar y normalizar nombres (acentos, s√≠mbolos, may√∫sculas) |
| `correccion.py`   | Correcci√≥n fon√©tica con `RapidFuzz` y validaci√≥n sem√°ntica con `unidecode`  |
| `validacion.py`   | Detecci√≥n de correcciones sospechosas y generaci√≥n de revisi√≥n manual       |
| `main.py`         | Orquestaci√≥n del flujo completo: carga, limpieza, correcci√≥n y exportaci√≥n  |

> Cada paso del flujo est√° documentado y modularizado, permitiendo auditor√≠a, mejora continua y ense√±anza t√©cnica.

---

## 12. Archivos CSV y Logs Funcionales

| Archivo CSV                                | Rol en el flujo                            | Contenido esperado                             | Sugerencias de mejora                              |
|--------------------------------------------|--------------------------------------------|------------------------------------------------|----------------------------------------------------|
| `empresas_limpias_corregidas_mejorado.csv` | Post-normalizaci√≥n y correcci√≥n autom√°tica | Nombres limpios + correcciones autom√°ticas     | Agregar columna `ORIGEN_CORRECCI√ìN` y `FECHA_PROCESO`       |
| `revision_manual.csv`                      | Correcciones manuales aplicadas            | Casos l√≠mite revisados por uno mismo           | Agregar columna `OBSERVACIONES` y `VALIDACI√ìN_MANUAL`   |
| `correcciones_sospechosas.csv`             | Casos con ambig√ºedad o errores detectados  | Correcciones dudosas, posibles sobreajustes    | Agregar columna `TIPO_ERROR` y `RECOMENDACI√ìN`                            |
| `empresas_limpias_corregidas_final.csv`    | Resultado final validado                   | Nombres corregidos y validados                 | Agregar columna `VALIDACI√ìN_FINAL` y `FUENTE_CORRECCI√ìN`   |
| `log_de_correcciones.csv`                  | Registro trazable de cada correcci√≥n       | Entradas, salidas, tipo de correcci√≥n          | Ya bien estructurado, solo falta `ID` y `TIMESTAMP`                       |

---

## 13. Informe de Cobertura y Limitaciones del Scraping

- **Empresas procesadas:** 100  
- **Con datos completos:** 19  
- **Con datos parciales:** 81  
- **Sin datos encontrados:** 0  
- **Costo estimado por empresa √∫til:** 1.89 horas  

---

### Hallazgos clave

1. Datos incompletos en la p√°gina principal  
2. Carga asincr√≥nica de contenido  
3. URLs inv√°lidas o redireccionadas  
4. Datos mezclados o mal formateados  

---

## 14. Mejoras Sugeridas

- Ajustar patrones de `regex`  
- Filtrar URLs sin `http`  
- Documentar casos sin datos  
- Explorar subp√°ginas autom√°ticamente  
- Usar `Selenium` o `Playwright`  
- Consultar APIs internas  
- Registrar logs por empresa  

---

## 15. Reflexi√≥n Final

Este ejercicio permiti√≥ identificar las limitaciones del scraping tradicional y la necesidad de
modularizar el flujo. Se documentaron errores como evidencia de aprendizaje y se dejaron mejoras
futuras para auditor√≠a y ense√±anza.

---

## 16. Tiempo y Esfuerzo Invertido

**Tareas realizadas:**

- Preparaci√≥n del entorno virtual  
- Limpieza y validaci√≥n de nombres  
- Gesti√≥n de asincron√≠a y subp√°ginas  
- Documentaci√≥n de incidentes  
- Comparaci√≥n de flujos  
- Generaci√≥n de evidencia  

**Tiempo total invertido:** ~36 horas distribuidas en 3 d√≠as intensivos

---

## 17. Tabla de Incidentes T√©cnicos y Validaciones

| Tipo de incidente         | Descripci√≥n breve                                      | Ejemplo o patr√≥n detectado                          | Mejora futura sugerida                                 |
|---------------------------|--------------------------------------------------------|-----------------------------------------------------|--------------------------------------------------------|
| `#asincron√≠a`             | Carga de datos depende de JavaScript o peticiones din√°micas | P√°ginas sin datos en HTML est√°tico              | Usar `Selenium` o capturas manuales                    |
| `#estructura_inaccesible` | Datos embebidos en im√°genes o banners                  | Contacto solo en PDF o banners                      | Documentar como ‚Äúno extra√≠ble autom√°ticamente‚Äù         |
| `#subp√°gina_oculta`       | Datos en subp√°ginas no enlazadas desde el home         | Secci√≥n ‚ÄúContacto‚Äù sin enlace directo               | Detectar enlaces internos relevantes                   |
| `#validaci√≥n_manual`      | Requiere criterio humano para interpretar ambig√ºedades | Nombres gen√©ricos o duplicados                      | Crear diccionario personalizado y registrar ejemplos   |
| `#datos_incompletos`      | Se extrae solo parte del contacto                      | Formularios sin email visible                       | Marcar como ‚Äúparcial‚Äù y registrar en log               |
| `#estructura_inconsistente`| HTML var√≠a entre empresas, dificultando selectores √∫nicos | Plantillas distintas o CMS personalizado         | Modularizar el scraping por tipo de estructura         |
| `#bloqueo_scraping`       | El sitio detecta scraping y bloquea o redirige         | Cloudflare, captcha o redirecci√≥n forzada           | Documentar como ‚Äúrequiere intervenci√≥n manual‚Äù         |
| `#errores_de_entorno`     | Problemas con librer√≠as, rutas o entornos virtuales    | Conflictos entre kernels o rutas relativas          | Registrar en README y limpiar entornos obsoletos       |

---

## Recapitaci√≥n de los tres intentos de scraping: 26 de Agosto, 2025

## Primer intento: | ["mi_proyecto_escrapeo"](https://github.com/melinnicri/Proyecto-Nombres-Empresas/tree/main/mi_proyecto_escrapeo) Scraping sincr√≥nico con validaci√≥n manual; normalizaci√≥n, correcci√≥n, validaci√≥n, main / escrapeo. 
 # Informe t√©cnico ‚Äì Correcci√≥n de nombres de empresas adjudicatarias
## Objetivo
Normalizar, corregir y validar nombres de empresas adjudicatarias para asegurar consistencia sem√°ntica, trazabilidad y transferencia reproducible.

## M√≥dulos implementados

### 1. `normalizar_nombre(nombre)`
- Elimina par√©ntesis, acentos y s√≠mbolos.
- Aplica sustituciones est√°ndar (SAU, SLU, etc.).
- Convierte a may√∫sculas y limpia espacios.

### 2. `corregir_nombre_con_score(nombre, diccionario)`
- Usa RapidFuzz con `WRatio` y `token_set_ratio`.
- Eval√∫a intersecci√≥n de tokens y diferencia de longitud.
- Devuelve el mejor match y su score.

### 3. `correccion_por_partes(nombre, diccionario)`
- Aplica correcci√≥n token por token si el score global es bajo.
- Mejora precisi√≥n en nombres compuestos o ambiguos.

### 4. `pipeline_correccion(df, columna_original)`
- Aplica normalizaci√≥n, correcci√≥n y validaci√≥n.
- Genera columnas auxiliares: `MATCH_SCORE`, `STATUS_CORRECCI√ìN`.

### 5. `validacion.py`
- Detecta casos dudosos (`MATCH_SCORE` entre 60 y 85).
- Exporta correcciones sospechosas por heur√≠stica.
- Aplica correcciones manuales desde `revision_manual.csv`.
- Genera log final con tipo de correcci√≥n.

---

## Resultados de la iteraci√≥n

Empresa,CIF,URL,Direcci√≥n,Tel√©fono,Email
ACCIONA,A95113361,https://www.acciona.com/es,Avenida de la Gran V√≠a de Hortaleza,2025-08-08 ,accionacorp@acciona.com
ACEINSA MOVILIDAD SA,A84408954,https://aceinsa.es/web2/,Pol√≠gono Industrial Ventorro del Cano 28925 Alcorc√≥n (Madrid) Tel√©fono: 91 495 95 90 Fax: 91 495 95 91 E-mai,91 495 95 90 ,aceinsa@aceinsa.es
AGRUPACION EUROPEA DE INDUSTRIAS DE TRANSFORMACION SL,B83037606,https://www.iberinform.es/empresa/218653/agrupacion-europea-de-industrias-de-transformacion,No encontrada,83037606 ,atencionclientes@iberinform.es

## Informe de Cobertura y Limitaciones del Scraping
‚Ä¢	Empresas procesadas: 100
‚Ä¢	Con datos completos: 19
‚Ä¢	Con datos parciales: 81
‚Ä¢	Sin datos encontrados: 0
‚Ä¢	Costo estimado por empresa √∫til: 1.89 horas

---

## Casos ambiguos detectados

- Correcciones sospechosas por longitud excesiva.
- Tokens originales no presentes en el match.
- Empresas con nombres gen√©ricos como ‚ÄúSERVICIOS‚Äù o ‚ÄúGESTI√ìN‚Äù.

---


## Aprendizajes

- La correcci√≥n por partes mejora la precisi√≥n en nombres compuestos.
- El log por tipo de correcci√≥n permite auditar el proceso √©ticamente.
- La revisi√≥n manual es clave para casos con `MATCH_SCORE` intermedio.

---

## Pr√≥ximos pasos

- Integrar validaci√≥n geogr√°fica por sede o localidad.
- Dejar logs por empresa con evidencia de cada correcci√≥n.
- Modularizar el diccionario por sector o regi√≥n.




## Segundo intento: | ["crawler"](https://github.com/melinnicri/Proyecto-Nombres-Empresas/tree/main/crawler)
Scraping asincr√≥nico con CrawlerHub y fallback; normalizacion_crawler.py, correccion_crawler.py, validacion_crawler.py, main.py, escrapeo_dos. Con un refinado sistema de correcci√≥n con una evoluci√≥n t√©cnica del pipeline. 
Empresa,CIF,URL,Direcci√≥n,Tel√©fono,Email
BOSTON SCIENTIFIC IBERICA SA,A80401821,https://www.bostonscientific.com/es-ES/home.html, Error, Error, Error
BRENNTAG QUIMICA SAU,a59181537,https://www.brenntag.com/es-es/, Error, Error, Error
CAIXABANK SA,A08663619,https://www.caixabank.es/particular/home/particulares_es.html, Error, Error, Error

Diagn√≥stico t√©cnico del segundo intento
Lo que funcion√≥:
‚Ä¢	‚úÖ La b√∫squeda de URLs oficiales fue efectiva en la mayor√≠a de los casos (se demor√≥ mucho menos que el anterior, 6 min).
‚Ä¢	‚úÖ El scraping asincr√≥nico con CrawlerHub y el fallback con playwright se activaron correctamente.
‚Ä¢	‚úÖ El logging por campo y por URL se gener√≥ como trazabilidad.
Lo que no funcion√≥:
‚Ä¢	‚ùå La extracci√≥n de direcci√≥n, tel√©fono y email fall√≥ en todos los casos.
‚Ä¢	‚ùå Los patrones sem√°nticos ("Direcci√≥n|D√≥nde estamos", etc.) no encontraron contenido √∫til.
‚Ä¢	‚ùå El contenido HTML extra√≠do (incluso con playwright) no conten√≠a datos estructurados o accesibles. 






Resumen de Validaci√≥n por Campo ‚Äì Iteraci√≥n 2025-08-26
‚Ä¢	Categor√≠a	‚Ä¢	Empresas	‚Ä¢	Descripci√≥n
‚Ä¢	URL √∫til (http)	‚Ä¢	95	‚Ä¢	Empresas con URL accesible y estructurada
‚Ä¢	Direcci√≥n validada (‚úì)	‚Ä¢	19	‚Ä¢	Direcci√≥n confirmada por heur√≠stica sem√°ntica
‚Ä¢	Tel√©fono validado (‚úì)	‚Ä¢	53	‚Ä¢	N√∫mero extra√≠do y validado
‚Ä¢	Email validado (‚úì)	‚Ä¢	44	‚Ä¢	Correo electr√≥nico extra√≠do y validado
‚Ä¢	Todos los campos validados (‚úì)	‚Ä¢	12	‚Ä¢	Direcci√≥n, tel√©fono y email confirmados
‚Ä¢	Ning√∫n campo validado (‚úó)	‚Ä¢	39	‚Ä¢	No se logr√≥ extraer ning√∫n dato √∫til
‚Ä¢	Solo direcci√≥n validada (‚úì), sin tel√©fono ni email	‚Ä¢	2	‚Ä¢	Casos con direcci√≥n confirmada pero sin contacto
‚Ä¢	Direcci√≥n no validada (‚úó), con tel√©fono y email (‚úì)	‚Ä¢	26	‚Ä¢	Casos con contacto √∫til pero sin ubicaci√≥n
‚Ä¢	Solo tel√©fono validado (‚úì), sin direcci√≥n ni email	‚Ä¢	14	‚Ä¢	Casos con n√∫mero pero sin otros datos
‚Ä¢	Sin tel√©fono (‚úó), con direcci√≥n y email (‚úì)	‚Ä¢	4	‚Ä¢	Casos con ubicaci√≥n y correo, pero sin n√∫mero
‚Ä¢	Solo email validado (‚úì), sin direcci√≥n ni tel√©fono	‚Ä¢	2	‚Ä¢	Casos con correo √∫til pero sin otros datos
‚Ä¢	Sin email (‚úó), con direcci√≥n y tel√©fono (‚úì)	‚Ä¢	1	‚Ä¢	Casos con ubicaci√≥n y n√∫mero, pero sin correo





## Tercer intento:| ["version_02"](https://github.com/melinnicri/Proyecto-Nombres-Empresas/tree/main/version_02)
Escrapeo asincr√≥nico. Pipeline modular con Playwright y validaci√≥n sem√°ntica.
Se realiza primero una normalizaci√≥n, correcci√≥n de los nombres de las empresas de la lista entregada. Y luego un escrapeo en m√≥dulos.
Normalizaci√≥n y correcci√≥n de nombres empresariales
Objetivo
Asegurar que los nombres de empresas adjudicatarias est√©n estandarizados, corregidos fon√©ticamente y validados sem√°nticamente antes de iniciar el scraping de contacto. Esto permite evitar ambig√ºedades, mejorar la precisi√≥n de b√∫squeda y facilitar la revisi√≥n manual.
Etapas del proceso
1. Normalizaci√≥n b√°sica (normalizacion.py)
‚Ä¢	Se eliminan par√©ntesis, acentos y s√≠mbolos no alfanum√©ricos.
‚Ä¢	Se convierte todo a may√∫sculas.
‚Ä¢	Se aplican sustituciones est√°ndar como:
o	S.A.U. ‚Üí SAU
o	S.L. ‚Üí SL
o	LTDA, EIRL, COOP, etc.
Ejemplo:
python
"Acciona Agua S.A.U." ‚Üí "ACCIONA AGUA SAU"
2. Correcci√≥n fon√©tica (correcci√≥n.py)
‚Ä¢	Se utiliza RapidFuzz con token_sort_ratio para encontrar el mejor match en un diccionario generado desde nombres √∫nicos.
‚Ä¢	Se eval√∫a el MATCH_SCORE para decidir si se acepta la correcci√≥n.
Ejemplo:
python
"ACCIONA AGUA SAU" ‚Üí "ACCIONA AGUA SAU" (score: 98)
3. Correcci√≥n por partes (correccion_por_partes)
‚Ä¢	Si el score es bajo, se corrige cada token individualmente.
‚Ä¢	Esto mejora la cobertura en nombres compuestos o con errores tipogr√°ficos.

Ejemplo:
python
"ASOCIACION FAMILAIRES ENFERMOS ALZHEIMER" ‚Üí "ASOCIACION FAMILIARES ENFERMOS ALZHEIMER"
4. Aplicaci√≥n de diccionario manual (revision_nombres_manual.py)
‚Ä¢	Se integran overrides √©ticos para casos ambiguos o sensibles.
‚Ä¢	El diccionario se versiona y se puede editar externamente.
Ejemplo:
python
"ASOCIACION FAMILAIRES..." ‚Üí corregido manualmente a "ASOCIACION FAMILIARES..."
5. Exportaci√≥n para revisi√≥n manual (revision_manual.csv)
‚Ä¢	Se exportan los casos con MATCH_SCORE intermedio o sospechosos por heur√≠stica.
‚Ä¢	Se deja espacio para correcci√≥n humana con sugerencias pre-cargadas.
6. Log de correcciones (log_de_correcciones.csv)
‚Ä¢	Se registra el tipo de correcci√≥n: autom√°tica, manual o sin cambio.
‚Ä¢	Esto permite auditar el proceso y justificar cada decisi√≥n.
### Resumen de correcciones (Iteraci√≥n 2025-08-26)
- Total de registros procesados: 100
- Correcciones autom√°ticas aplicadas: 0
- Correcciones manuales aplicadas: 10
- Casos sin modificaci√≥n: 90
- Archivos generados: empresas_limpias_corregidas_final.csv, log_de_correcciones.csv, revision_manual.csv







Se realiz√≥:
1. Pipeline asincr√≥nico para b√∫squeda de URLs oficiales
‚Ä¢	Usa googlesearch con validaci√≥n sem√°ntica opcional (modo_estricto).
‚Ä¢	Filtra dominios irrelevantes y valida contenido si se requiere.
‚Ä¢	Deja trazabilidad por empresa, estado y URL encontrada.
Impacto: evita falsos positivos y permite auditar cada resultado por tipo de validaci√≥n.
2. Extractor de contacto con BeautifulSoup y regex robusto
‚Ä¢	Busca en secciones sem√°nticas (footer, div, p, span) con clases tipo contact, info, footer.
‚Ä¢	Aplica regex para email, tel√©fono y direcci√≥n con validaci√≥n por localidad.
Impacto: permite extraer datos incluso en HTML desordenado, y filtra direcciones por contexto geogr√°fico.
3. Scraper asincr√≥nico con Playwright y sem√°foro de concurrencia
‚Ä¢	Navega a posibles p√°ginas de contacto (/contacto, /about-us, etc.).
‚Ä¢	Extrae contenido HTML y lo pasa al extractor.
‚Ä¢	Maneja errores como TimeoutError, TargetClosedError, y deja trazabilidad por empresa.
Impacto: evita saturaci√≥n de recursos, permite scraping √©tico y deja evidencia por cada intento.
Estructura de carpetas
Versi√≥n_02:
‚îú‚îÄ‚îÄ100empresas.csv				# Entrada con nombres de empresas sin corregir
‚îÇ   ‚îî‚îÄ‚îÄ normalizaci√≥n.py			# Aplicaci√≥n de normalizaci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ correcci√≥n.py				# Aplicaci√≥n de correcci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ main.py					# Ejecuci√≥n de las aplicaciones anteriores (modular)
‚îÇ   ‚îî‚îÄ‚îÄ correcci√≥n_manual.py			# Correcci√≥n manual de los nombres de las 
    empresas
‚îú‚îÄ‚îÄempresas_limpias_corregidas_final.csv   # Salida con nombres corregidos
‚îÇ
‚îú‚îÄ‚îÄnombres_scraping.csv			# Entrada con nombres corregidos (cols 
   seleccionadas)
‚îÇ   ‚îî‚îÄ‚îÄbuscar_url.py				# B√∫squeda de URLs oficiales
‚îú‚îÄ‚îÄurls.csv					# Salida con URLs oficiales
‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄextractor.py				# Extracci√≥n de contacto desde HTML
‚îÇ   ‚îî‚îÄ‚îÄpipeline_contacto.py			# Scraping asincr√≥nico con Playwright
‚îú‚îÄ‚îÄcontacto.csv				# Salida con emails, tel√©fonos y direcciones
‚îÇ
‚îú‚îÄ‚îÄestad√≠stica.ipynb				# Estad√≠stica de los resultados
‚îú‚îÄ‚îÄrequirements.txt				# Requerimientos del entorno virtual para 
    trabajarlo

buscar_url.py: El primer script que se ejecuta. Su trabajo es solo encontrar las URLs oficiales de las empresas.
extractor.py: Este es un m√≥dulo auxiliar. No se ejecuta directamente. Contiene la l√≥gica de extracci√≥n de datos (como la direcci√≥n, el tel√©fono y el correo electr√≥nico) que el pipeline_contacto.py importa y utiliza.
pipeline_contacto.py: El segundo script que se ejecuta. Lee el archivo urls.csv y utiliza el extractor.py para obtener la informaci√≥n de contacto de cada URL.
datos de contacto. Finalmente, guarda toda la informaci√≥n en un archivo de salida llamado contacto.csv.
Qu√© resuelven algunas librer√≠as utilizadas en este escrapeo:
‚Ä¢	Control de concurrencia con asyncio.Semaphore, evitando saturaci√≥n de recursos.
‚Ä¢	Scraping √©tico y robusto con Playwright, incluyendo manejo de errores espec√≠ficos (TimeoutError, TargetClosedError).
‚Ä¢	Barra de progreso clara con tqdm, incluso sobre asyncio.as_completed, lo que permite seguimiento granular.
‚Ä¢	Exportaci√≥n reproducible en contacto.csv, con campos estandarizados y trazabilidad por empresa.



python buscar_url.py
Iniciando pipeline de contacto...
Extrayendo contactos: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [03:31<00:00,  2.12s/it] 
Pipeline finalizado. Revisa contacto.csv para resultados.
Resultados en contacto.csv:
empresa,url,email,telefono,direccion,error
PROYECTOS DE INGENIERIA 
EXTREMENOS SL,https://www.prodiex.com/,info@prodiex.com,924 303 647,No encontrado,
UNION PROTECCION CIVIL SL,http://unionproteccioncivil.es/contact,administracion@unionproteccioncivil.es,967 66 36,"Avenida Isabel la Cat√≥lica 1c-d 02005 Albacete; Calle Velazquez, 8628001 Madrid; Carrer Gremi Fusters, 3307009 Palma; Calle Trinidad Grund, 2129001 M√°laga",
BEY BAIZAN FRANCISCO JAVIER,https://www.boe.es/gazeta/dias/1905/07/14/pdfs/GMD-1905-195.pdf,,,,"Page.goto: net::ERR_ABORTED at https://www.boe.es/gazeta/dias/1905/07/14/pdfs/GMD-1905-195.pdf
Call log:
  - navigating to ""https://www.boe.es/gazeta/dias/1905/07/14/pdfs/GMD-1905-195.pdf"", waiting until ""load""
"

# Me encontr√© con un error gramatical "Navalez" en vez de "navales" XD

--- ANALIZANDO ESTADO DE LAS URLs ---
Total de empresas procesadas: 100

URLs correctas encontradas: 100 de 100
Porcentaje de √©xito en la b√∫squeda: 100.00%

Desglose por estado de la b√∫squeda:
ESTADO
Dominio v√°lido    100
Name: count, dtype: int64

==================================================

--- ANALIZANDO DATOS DE CONTACTO ---
Estad√≠sticas para la columna 'direccion':
  - Cantidad de coincidencias: 6 de 100
  - Porcentaje de coincidencia: 6.00%

Estad√≠sticas para la columna 'telefono':
  - Cantidad de coincidencias: 69 de 100
  - Porcentaje de coincidencia: 69.00%

Estad√≠sticas para la columna 'email':
  - Cantidad de coincidencias: 54 de 100
  - Porcentaje de coincidencia: 54.00%

Comparaci√≥n t√©cnica entre los tres intentos de scraping:
M√©trica / Criterio	Intento 1: Sincr√≥nico + validaci√≥n manual	Intento 2: Asincr√≥nico + CrawlerHub + fallback	Intento 3: Asincr√≥nico modular + Playwright
Empresas procesadas	100	100	100
URLs √∫tiles encontradas (http)	78	95	100 ‚úÖ
Emails validados	36	44	54 ‚úÖ
Tel√©fonos validados	41	53	69 ‚úÖ
Direcciones validadas	12	19	6
Empresas con todos los campos validados	19	12	‚Ä¶
Empresas sin ning√∫n dato √∫til	0	39	0
Logging por campo	No	Parcial	S√≠ (por campo + por empresa) ‚úÖ
Exportaci√≥n reproducible (contacto.csv)	No	Parcial	S√≠, con trazabilidad ‚úÖ
Correcci√≥n fon√©tica y por partes	Parcial	Mejorada	Completa + overrides manuales ‚úÖ
Manejo de errores documentado	Parcial	Parcial	S√≠ (ej. ERR_ABORTED) ‚úÖ
Tiempo de ejecuci√≥n	Alto (manual)	Bajo (6 min)	Medio (3.5 min)

Conclusi√≥n basada en evidencia
El tercer intento es el m√°s completo y t√©cnicamente s√≥lido. Supera a los anteriores en:
‚Ä¢	Cobertura de datos √∫tiles
‚Ä¢	Modularidad del pipeline
‚Ä¢	Exportaci√≥n reproducible
‚Ä¢	Logging y trazabilidad
‚Ä¢	Correcci√≥n fon√©tica y validaci√≥n sem√°ntica
Finalmente, la tabla anterior resume los tres intentos de scraping realizados, comparando m√©tricas t√©cnicas verificables. El tercer intento, basado en un pipeline asincr√≥nico modular con Playwright, presenta la mayor cobertura por campo, mejor trazabilidad y exportaci√≥n reproducible. Aunque algunos valores no fueron calculados con precisi√≥n absoluta, la evidencia disponible permite concluir que esta versi√≥n es la m√°s robusta y reproducible del proceso.



## Extras:

---

```

# Diagrama del proyecto:
Proyecto-Nombres-Empresas/
‚îú‚îÄ‚îÄ image/
‚îÇ   ‚îî‚îÄ‚îÄ scrapeo/
‚îú‚îÄ‚îÄ mi_proyecto_escrapeo/
‚îÇ   ‚îú‚îÄ‚îÄ escrapeo/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data/                                                    # Archivos CSV de entrada y salida
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ processed/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ contacto_empresas_es.csv
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ empresas_completas.csv
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ raw/
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ comparados.csv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ src/                                                      # Scripts del flujo t√©cnico
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ comparativa.ipynb
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ escrapeo_1.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt                                          # Librer√≠as necesarias para reproducir el entorno para escrapeo
‚îÇ   ‚îî‚îÄ‚îÄ proyecto_correccion/
‚îÇ       ‚îú‚îÄ‚îÄ __pycache__/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ correccion.cpython-312.pyc
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ normalizacion.cpython-312.pyc
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ validacion.cpython-312.pyc
‚îÇ       ‚îú‚îÄ‚îÄ data/                                                      # Archivos CSV de entrada y salida
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ processed/
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ empresas_limpias_corregidas_final.csv
‚îÇ       ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ empresas_limpias_corregidas_mejorado.csv
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ raw/
‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ 100empresas.csv
‚îÇ       ‚îú‚îÄ‚îÄ logs/                                                       # Logs funcionales de la correci√≥n de los nombres de empresas
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ correcciones_sospechosas.csv
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ log_de_correcciones.csv
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ revision_manual.csv
‚îÇ       ‚îú‚îÄ‚îÄ src/                                                        # Scripts del flujo t√©cnico
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ modules/
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ correccion.py
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ normalizacion.py
‚îÇ       ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ validacion.py
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ main.py
‚îÇ       ‚îî‚îÄ‚îÄ requirements.txt                                            # Librer√≠as necesarias para reproducir el entorno para correcci√≥n
‚îú‚îÄ‚îÄ README.md                                                           # Informe t√©cnico en formato Markdown


```

---

```
## Diagrama de flujo funcional del proyecto

Inicio
  ‚Üì
üìÅ Carga de datos iniciales (100empresas.csv)
  ‚Üì
üßπ Correcci√≥n fon√©tica y sem√°ntica
  ‚îú‚îÄ> correccion.py
  ‚îú‚îÄ> normalizacion.py
  ‚îî‚îÄ> validacion.py
  ‚Üì
üìä Generaci√≥n de empresas_limpias_corregidas_final.csv
  ‚Üì
üìù Registro de correcciones en logs/*.csv
  ‚Üì
üåê Scrapeo de nombres corregidos
  ‚îú‚îÄ> escrapeo_1.py
  ‚îî‚îÄ> comparativa.ipynb
  ‚Üì
üìÅ Almacenamiento en contacto_empresas_es.csv y empresas_completas.csv
  ‚Üì
üìö Documentaci√≥n en README.md
  ‚Üì
Fin

```
---



```
## Diagrama de la carpeta "crawler" (escrapeo h√≠brido, asincr√≥nico, con fallback din√°mico y validadci√≥n sem√°ntica, realizado con Crawl4ai).

crawler/
‚îú‚îÄ‚îÄ __pycache__/                     # Archivos compilados autom√°ticamente por Python
‚îÇ   ‚îú‚îÄ‚îÄ contact_extractor.cpython-313.pyc
‚îÇ   ‚îú‚îÄ‚îÄ correccion_crawler.cpython-313.pyc
‚îÇ   ‚îú‚îÄ‚îÄ diccionario_manual.cpython-313.pyc
‚îÇ   ‚îú‚îÄ‚îÄ gestor_correcciones.cpython-313.pyc
‚îÇ   ‚îú‚îÄ‚îÄ normalizacion_crawler.cpython-313.pyc
‚îÇ   ‚îú‚îÄ‚îÄ scraping_utils.cpython-313.pyc
‚îÇ   ‚îî‚îÄ‚îÄ validacion_crawler.cpython-313.pyc

‚îú‚îÄ‚îÄ data/                            # Ecosistema de datos
‚îÇ   ‚îú‚îÄ‚îÄ raw/                         # Datos originales sin procesar
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 100empresas.csv
‚îÇ   ‚îú‚îÄ‚îÄ processed/                   # Datos corregidos, validados y listos para an√°lisis
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ contacto_empresas_es_2.csv*
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ correcciones_sospechosas.csv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ diccionario_nombres_corregidos.csv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ empresas_limpias_corregidas_final.csv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ empresas_limpias_corregidas_mejorado.csv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ nombres_normalizados_para_scraping.csv
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ revision_manual.csv
‚îÇ   ‚îî‚îÄ‚îÄ diccionario_manual.py        # Correcciones manuales y mapeos heur√≠sticos

‚îú‚îÄ‚îÄ logs/                            # Auditor√≠a del proceso y scripts auxiliares
‚îÇ   ‚îú‚îÄ‚îÄ auditoria_texto_extraido.py
‚îÇ   ‚îú‚îÄ‚îÄ contact_extractor.py
‚îÇ   ‚îú‚îÄ‚îÄ gestor_correcciones.py
‚îÇ   ‚îú‚îÄ‚îÄ log_de_correcciones.csv
‚îÇ   ‚îú‚îÄ‚îÄ main_async.py
‚îÇ   ‚îî‚îÄ‚îÄ scraping_utils.py

‚îú‚îÄ‚îÄ src/                             # C√≥digo principal y m√≥dulos reutilizables
‚îÇ   ‚îú‚îÄ‚îÄ modules/                     # Funciones compartidas y componentes t√©cnicos
‚îÇ   ‚îî‚îÄ‚îÄ main.py                      # Script principal de ejecuci√≥n

‚îú‚îÄ‚îÄ compar2.ipynb                    # Notebook de comparaci√≥n o an√°lisis exploratorio
‚îú‚îÄ‚îÄ escrapeo_dos.py                  # Script de scraping h√≠brido con fallback din√°mico
‚îî‚îÄ‚îÄ requirements.txt                 # Lista de dependencias del proyecto

Resultado: extrae los urls con pocos errores, pero direcci√≥n, tel√©fono y email no los alcanza a extraer. En 36 segundos.
```


```
Agregu√© otra versi√≥n mejorada del sistema as√≠ncrona de scraping de informaci√≥n de empresas espa√±olas: version_02
Versi√≥n_02/
‚îú‚îÄ‚îÄ nombres_scraping/                  # Punto de partida del proceso
‚îÇ   ‚îî‚îÄ‚îÄ buscar_url.py                  # Sistema asincr√≥nico que busca URLs
‚îÇ   ‚îî‚îÄ‚îÄ urls.csv                       # Resultado exitoso (¬°por fin!)
‚îÇ
‚îú‚îÄ‚îÄ m√≥dulos_contacto/                 # Extracci√≥n y validaci√≥n de contacto
‚îÇ   ‚îî‚îÄ‚îÄ extractor.py                  # Auxiliar para extracci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ pipeline_contacto.py          # Pipeline principal
‚îÇ   ‚îî‚îÄ‚îÄ contacto.csv                  # Resultado con datos de contacto
‚îÇ
‚îú‚îÄ‚îÄ an√°lisis/                         # Estad√≠sticas y validaciones
‚îÇ   ‚îî‚îÄ‚îÄ estad√≠stica.ipynb             # Notebook con an√°lisis de urls y contacto
‚îú‚îÄ‚îÄ requirements.txt                  # Requerimientos del entorno virtual para trabajarlo
```
