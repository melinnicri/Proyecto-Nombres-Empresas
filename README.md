# Prueba T√©cnica: Scraping de Informaci√≥n en P√°gina Web

**Extracci√≥n automatizada de datos de contacto empresarial desde fuentes p√∫blicas**  
**Autora:** Amelia Cristina Herrera Brice√±o  
**Rol:** Data Analyst & Scientist en transici√≥n hacia BI y automatizaci√≥n  
**Entorno de trabajo:** Python ¬∑ Pandas ¬∑ BeautifulSoup ¬∑ Regex ¬∑ CSV ¬∑ Exploraci√≥n reproducible  
**Fecha de entrega:** 15 Agosto 2025  


<p align="center"><img src="https://github.com/melinnicri/Proyecto-Nombres-Empresas/blob/main/images/Accidentes.jpg"></p>
---

## 1. Problema

Se cuenta con listados de empresas (nombre y CIF), pero se carece de sus datos de contacto directo (emails corporativos, tel√©fonos, direcciones). Para establecer comunicaci√≥n comercial, es necesario localizar y extraer autom√°ticamente esta informaci√≥n desde sus sitios web oficiales u otras fuentes p√∫blicas disponibles en internet.

---

## 2. Soluci√≥n T√©cnica

Desarrollar un proceso de scraping web que permita obtener datos de contacto empresariales de forma automatizada a partir de los nombres de las empresas.

---

## 3. Alcance de la Prueba

- **Dataset:** Muestra de 50‚Äì100 empresas espa√±olas (nombre + CIF)
- **Proceso esperado:**
  1. Localizar la web corporativa oficial de cada empresa
  2. Extraer datos de contacto mediante scraping web
- **Datos objetivo:**
  - URL oficial
  - Email corporativo principal (info@, contacto@, comercial@)
  - Tel√©fono de contacto
  - Direcci√≥n postal completa
- **Entregables:**
  - Archivo CSV/Excel con los datos obtenidos
  - Documentaci√≥n t√©cnica del proceso
  - Registro de limitaciones (robots.txt, captchas, etc.)
  - Tiempo estimado de procesamiento por empresa
  - Coste estimado del proceso
  - Video explicativo del flujo t√©cnico

---

## 4. Implementaci√≥n T√©cnica

- Scraping sincr√≥nico reproducible: `requests` + `BeautifulSoup` + `regex`
- Limpieza post-scraping: DataFrames exportables, validaci√≥n de encoding y estructura
- Exportaci√≥n: Archivos CSV listos para an√°lisis y visualizaci√≥n

---

## 5. M√©tricas y Evaluaci√≥n

- **Tasa de √©xito:** porcentaje de empresas con datos completos
- **Limitaciones encontradas:** robots.txt, captchas, p√°ginas sin datos visibles
- **Tiempo estimado por empresa:** medido con barra de progreso
- **Coste estimado:** bajo en entorno local; escalable con APIs

---

## 6. Video Explicativo

üé• Se graba un video mostrando el flujo, explicando el c√≥digo y los resultados obtenidos.  
_Pendiente de edici√≥n y subida._

---

## 7. Escalabilidad

Para flujos masivos, se propone una versi√≥n asincr√≥nica con `aiohttp` o `crawl4ai`, que mejora el rendimiento y permite scraping concurrente.

---

## 8. Comparativa de M√©todos de B√∫squeda de URLs Oficiales

| M√©todo                    | Gratuito | L√≠mite        | Costo estimado | Ventajas                         | Desventajas                         |
|---------------------------|----------|---------------|----------------|----------------------------------|-------------------------------------|
| googlesearch (scraping)   | S√≠       | No oficial    | $0             | Sin registro, ideal para pruebas | Bloqueos, resultados no estructurados |
| SerpAPI                   | S√≠ (250) | Pago desde $50| ~$50           | JSON limpio, sin bloqueo         | Requiere API Key, coste mensual     |
| Zenserp                   | S√≠ (50)  | Pago desde $29| ~$29           | F√°cil de usar, resultados precisos| Menor volumen gratuito              |
| Google Custom Search API  | S√≠ (100) | $5/1000       | ~$5            | Oficial, configurable            | Requiere configuraci√≥n previa       |
| Bing Search API           | S√≠ (1000)| Pago desde $3 | ~$3            | Econ√≥mica, buena cobertura       | Menor precisi√≥n en empresas locales |

---

## 9. Recomendaci√≥n Final

Para esta prueba t√©cnica (50‚Äì100 empresas), se recomienda utilizar `googlesearch` por su simplicidad y coste cero. Para escalar el proceso, se sugiere evaluar APIs comerciales como `SerpAPI` y adoptar scraping asincr√≥nico con `aiohttp` o `crawl4ai`.

---

## 10. Correcci√≥n Fon√©tica y Sem√°ntica de Nombres Empresariales

Objetivo: corregir nombres empresariales con validaci√≥n fon√©tica (`rapidfuzz`) y sem√°ntica (`unidecode`), asegurando trazabilidad y limpieza formal.

### Ejemplo de correcci√≥n fon√©tica

```python
import re
from unidecode import unidecode

def normalizar_nombre(nombre):
    nombre = unidecode(nombre)
    nombre = re.sub(r'[^\w\s]', '', nombre)
    nombre = nombre.strip().upper()
    return nombre

nombre_1 = "Compa√±√≠a ABC S.A."
nombre_2 = "Compania - ABC, SA"

print(normalizar_nombre(nombre_1))  # COMPANIA ABC SA
print(normalizar_nombre(nombre_2))  # COMPANIA ABC SA

## 11. Flujo de Correcci√≥n y Validaci√≥n

Este m√≥dulo se encarga de limpiar, corregir y validar los nombres empresariales antes del scraping, asegurando trazabilidad y coherencia sem√°ntica.

| Archivo           | Prop√≥sito principal                                                  |
|-------------------|----------------------------------------------------------------------|
| `normalizacion.py`| Funciones para limpiar y normalizar nombres (acentos, s√≠mbolos, may√∫sculas) |
| `correccion.py`   | Correcci√≥n fon√©tica con `RapidFuzz` y validaci√≥n sem√°ntica con `unidecode` |
| `validacion.py`   | Detecci√≥n de correcciones sospechosas y generaci√≥n de revisi√≥n manual |
| `main.py`         | Orquestaci√≥n del flujo completo: carga, limpieza, correcci√≥n y exportaci√≥n |

> Cada paso del flujo est√° documentado y modularizado, permitiendo auditor√≠a, mejora continua y ense√±anza t√©cnica.

## 12. Archivos CSV y Logs Funcionales

| Archivo CSV                              | Rol en el flujo                          | Contenido esperado                             | Sugerencias de mejora                                      |
|------------------------------------------|------------------------------------------|------------------------------------------------|-------------------------------------------------------------|
| `empresas_limpias_corregidas_mejorado.csv` | Post-normalizaci√≥n y correcci√≥n autom√°tica | Nombres limpios + correcciones autom√°ticas     | Agregar columna `ORIGEN_CORRECCI√ìN` y `FECHA_PROCESO`       |
| `revision_manual.csv`                    | Correcciones manuales aplicadas          | Casos l√≠mite revisados por uno mismo           | Agregar columna `OBSERVACIONES` y `VALIDACI√ìN_MANUAL`       |
| `correcciones_sospechosas.csv`           | Casos con ambig√ºedad o errores detectados| Correcciones dudosas, posibles sobreajustes    | Agregar columna `TIPO_ERROR` y `RECOMENDACI√ìN`              |
| `empresas_limpias_corregidas_final.csv`  | Resultado final validado                 | Nombres corregidos y validados                 | Agregar columna `VALIDACI√ìN_FINAL` y `FUENTE_CORRECCI√ìN`    |
| `log_de_correcciones.csv`                | Registro trazable de cada correcci√≥n     | Entradas, salidas, tipo de correcci√≥n          | Ya bien estructurado, solo falta `ID` y `TIMESTAMP`         |

---

## 13. Informe de Cobertura y Limitaciones del Scraping

- **Empresas procesadas:** 100  
- **Con datos completos:** 19  
- **Con datos parciales:** 81  
- **Sin datos encontrados:** 0  
- **Costo estimado por empresa √∫til:** 1.89 horas  

### Hallazgos clave

1. Datos incompletos en la p√°gina principal  
2. Carga asincr√≥nica de contenido  
3. URLs inv√°lidas o redireccionadas  
4. Datos mezclados o mal formateados  

---

## 14. Mejoras Sugeridas

- Ajustar patrones de `regex`  
- Filtrar URLs sin `http`  
- Documentar casos sin datos  
- Explorar subp√°ginas autom√°ticamente  
- Usar `Selenium` o `Playwright`  
- Consultar APIs internas  
- Registrar logs por empresa  

---

## 15. Reflexi√≥n Final

Este ejercicio permiti√≥ identificar las limitaciones del scraping tradicional y la necesidad de modularizar el flujo. Se documentaron errores como evidencia de aprendizaje y se dejaron mejoras futuras para auditor√≠a y ense√±anza.

---

## 16. Tiempo y Esfuerzo Invertido

**Tareas realizadas:**

- Preparaci√≥n del entorno virtual  
- Limpieza y validaci√≥n de nombres  
- Gesti√≥n de asincron√≠a y subp√°ginas  
- Documentaci√≥n de incidentes  
- Comparaci√≥n de flujos  
- Generaci√≥n de evidencia  

**Tiempo total invertido:** ~36 horas distribuidas en 3 d√≠as intensivos

---

## 17. Tabla de Incidentes T√©cnicos y Validaciones

| Tipo de incidente         | Descripci√≥n breve                                      | Ejemplo o patr√≥n detectado                          | Mejora futura sugerida                                 |
|---------------------------|--------------------------------------------------------|-----------------------------------------------------|--------------------------------------------------------|
| `#asincron√≠a`             | Carga de datos depende de JavaScript o peticiones din√°micas | P√°ginas sin datos en HTML est√°tico              | Usar `Selenium` o capturas manuales                    |
| `#estructura_inaccesible` | Datos embebidos en im√°genes o banners                  | Contacto solo en PDF o banners                      | Documentar como ‚Äúno extra√≠ble autom√°ticamente‚Äù         |
| `#subp√°gina_oculta`       | Datos en subp√°ginas no enlazadas desde el home         | Secci√≥n ‚ÄúContacto‚Äù sin enlace directo               | Detectar enlaces internos relevantes                   |
| `#validaci√≥n_manual`      | Requiere criterio humano para interpretar ambig√ºedades | Nombres gen√©ricos o duplicados                      | Crear diccionario personalizado y registrar ejemplos   |
| `#datos_incompletos`      | Se extrae solo parte del contacto                      | Formularios sin email visible                       | Marcar como ‚Äúparcial‚Äù y registrar en log               |
| `#estructura_inconsistente`| HTML var√≠a entre empresas, dificultando selectores √∫nicos | Plantillas distintas o CMS personalizado         | Modularizar el scraping por tipo de estructura         |
| `#bloqueo_scraping`       | El sitio detecta scraping y bloquea o redirige         | Cloudflare, captcha o redirecci√≥n forzada           | Documentar como ‚Äúrequiere intervenci√≥n manual‚Äù         |
| `#errores_de_entorno`     | Problemas con librer√≠as, rutas o entornos virtuales    | Conflictos entre kernels o rutas relativas          | Registrar en README y limpiar entornos obsoletos       |


