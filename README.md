# Prueba TÃ©cnica: Scraping de InformaciÃ³n en PÃ¡gina Web

**ExtracciÃ³n automatizada de datos de contacto empresarial desde fuentes pÃºblicas**  
**Autora:** Amelia Cristina Herrera BriceÃ±o  
**Rol:** Data Analyst & Scientist en transiciÃ³n hacia BI y automatizaciÃ³n  
**Entorno de trabajo:** Python Â· Pandas Â· BeautifulSoup Â· Regex Â· CSV Â· ExploraciÃ³n reproducible  
**Fecha de entrega:** 15 Agosto 2025  

<p align="center"><img src="https://github.com/melinnicri/Proyecto-Nombres-Empresas/blob/main/image/scrapeo.png"></p>

---

## 1. Problema

Se cuenta con listados de empresas (nombre y CIF), pero se carece de sus datos de contacto directo 
(emails corporativos, telÃ©fonos, direcciones). Para establecer comunicaciÃ³n comercial, es necesario 
localizar y extraer automÃ¡ticamente esta informaciÃ³n desde sus sitios web oficiales u otras fuentes 
pÃºblicas disponibles en internet.

---

## 2. SoluciÃ³n TÃ©cnica

Desarrollar un proceso de scraping web que permita obtener datos de contacto empresariales de forma 
automatizada a partir de los nombres de las empresas.

---

## 3. Alcance de la Prueba

- **Dataset:** Muestra de 50â€“100 empresas espaÃ±olas (nombre + CIF)
- **Proceso esperado:**
  1. Localizar la web corporativa oficial de cada empresa
  2. Extraer datos de contacto mediante scraping web
- **Datos objetivo:**
  - URL oficial
  - Email corporativo principal (info@, contacto@, comercial@)
  - TelÃ©fono de contacto
  - DirecciÃ³n postal completa
- **Entregables:**
  - Archivo CSV/Excel con los datos obtenidos
  - DocumentaciÃ³n tÃ©cnica del proceso
  - Registro de limitaciones (robots.txt, captchas, etc.)
  - Tiempo estimado de procesamiento por empresa
  - Coste estimado del proceso
  - Video explicativo del flujo tÃ©cnico

---

## 4. ImplementaciÃ³n TÃ©cnica

- Scraping sincrÃ³nico reproducible: `requests` + `BeautifulSoup` + `regex`
- Limpieza post-scraping: DataFrames exportables, validaciÃ³n de encoding y estructura
- ExportaciÃ³n: Archivos CSV listos para anÃ¡lisis y visualizaciÃ³n

---

## 5. MÃ©tricas y EvaluaciÃ³n

- **Tasa de Ã©xito:** porcentaje de empresas con datos completos
- **Limitaciones encontradas:** robots.txt, captchas, pÃ¡ginas sin datos visibles
- **Tiempo estimado por empresa:** medido con barra de progreso
- **Coste estimado:** bajo en entorno local; escalable con APIs

---

## 6. Video Explicativo

ğŸ¥ Se graba un video mostrando el flujo, explicando el cÃ³digo y los resultados obtenidos.  
_Pendiente de ediciÃ³n y subida._

---

## 7. Escalabilidad

Para flujos masivos, se propone una versiÃ³n asincrÃ³nica con `aiohttp` o `crawl4ai`, que mejora el 
rendimiento y permite scraping concurrente.

---

## 8. Comparativa de MÃ©todos de BÃºsqueda de URLs Oficiales


| MÃ©todo                    | Gratuito | LÃ­mite         | Costo estimado | Ventajas                            | Desventajas                           |
|---------------------------|----------|----------------|----------------|-------------------------------------|---------------------------------------|
| googlesearch (scraping)   | SÃ­       | No oficial     | $0             | Sin registro, ideal para pruebas    | Bloqueos, resultados no estructurados |
| SerpAPI                   | SÃ­ (250) | Pago desde $50 | ~$50           | JSON limpio, sin bloqueo            | Requiere API Key, coste mensual       |
| Zenserp                   | SÃ­ (50)  | Pago desde $29 | ~$29           | FÃ¡cil de usar, resultados precisos  | Menor volumen gratuito                |
| Google Custom Search API  | SÃ­ (100) | $5/1000        | ~$5            | Oficial, configurable               | Requiere configuraciÃ³n previa         |
| Bing Search API           | SÃ­ (1000)| Pago desde $3  | ~$3            | EconÃ³mica, buena cobertura          | Menor precisiÃ³n en empresas locales   |

---

## 9. RecomendaciÃ³n Final

Para esta prueba tÃ©cnica (50â€“100 empresas), se recomienda utilizar `googlesearch` por su simplicidad 
y coste cero. Para escalar el proceso, se sugiere evaluar APIs comerciales como `SerpAPI` y adoptar 
scraping asincrÃ³nico con `aiohttp` o `crawl4ai`.

---

## 10. CorrecciÃ³n FonÃ©tica y SemÃ¡ntica de Nombres Empresariales

Objetivo: corregir nombres empresariales con validaciÃ³n fonÃ©tica (`rapidfuzz`) y semÃ¡ntica (`unidecode`), 
asegurando trazabilidad y limpieza formal.

### Ejemplo de correcciÃ³n fonÃ©tica

```python
import re
from unidecode import unidecode

def normalizar_nombre(nombre):
    nombre = unidecode(nombre)
    nombre = re.sub(r'[^\w\s]', '', nombre)
    nombre = nombre.strip().upper()
    return nombre

nombre_1 = "CompaÃ±Ã­a ABC S.A."
nombre_2 = "Compania - ABC, SA"

print(normalizar_nombre(nombre_1))  # COMPANIA ABC SA
print(normalizar_nombre(nombre_2))  # COMPANIA ABC SA
```

---

## 11. Flujo de CorrecciÃ³n y ValidaciÃ³n

Este mÃ³dulo se encarga de limpiar, corregir y validar los nombres empresariales antes del scraping,
asegurando trazabilidad y coherencia semÃ¡ntica.

| Archivo           | PropÃ³sito principal                                                         |
|-------------------|-----------------------------------------------------------------------------|
| `normalizacion.py`| Funciones para limpiar y normalizar nombres (acentos, sÃ­mbolos, mayÃºsculas) |
| `correccion.py`   | CorrecciÃ³n fonÃ©tica con `RapidFuzz` y validaciÃ³n semÃ¡ntica con `unidecode`  |
| `validacion.py`   | DetecciÃ³n de correcciones sospechosas y generaciÃ³n de revisiÃ³n manual       |
| `main.py`         | OrquestaciÃ³n del flujo completo: carga, limpieza, correcciÃ³n y exportaciÃ³n  |

> Cada paso del flujo estÃ¡ documentado y modularizado, permitiendo auditorÃ­a, mejora continua y enseÃ±anza tÃ©cnica.

---

## 12. Archivos CSV y Logs Funcionales

| Archivo CSV                                | Rol en el flujo                            | Contenido esperado                             | Sugerencias de mejora                              |
|--------------------------------------------|--------------------------------------------|------------------------------------------------|----------------------------------------------------|
| `empresas_limpias_corregidas_mejorado.csv` | Post-normalizaciÃ³n y correcciÃ³n automÃ¡tica | Nombres limpios + correcciones automÃ¡ticas     | Agregar columna `ORIGEN_CORRECCIÃ“N` y `FECHA_PROCESO`       |
| `revision_manual.csv`                      | Correcciones manuales aplicadas            | Casos lÃ­mite revisados por uno mismo           | Agregar columna `OBSERVACIONES` y `VALIDACIÃ“N_MANUAL`   |
| `correcciones_sospechosas.csv`             | Casos con ambigÃ¼edad o errores detectados  | Correcciones dudosas, posibles sobreajustes    | Agregar columna `TIPO_ERROR` y `RECOMENDACIÃ“N`                            |
| `empresas_limpias_corregidas_final.csv`    | Resultado final validado                   | Nombres corregidos y validados                 | Agregar columna `VALIDACIÃ“N_FINAL` y `FUENTE_CORRECCIÃ“N`   |
| `log_de_correcciones.csv`                  | Registro trazable de cada correcciÃ³n       | Entradas, salidas, tipo de correcciÃ³n          | Ya bien estructurado, solo falta `ID` y `TIMESTAMP`                       |

---

## 13. Informe de Cobertura y Limitaciones del Scraping

- **Empresas procesadas:** 100  
- **Con datos completos:** 19  
- **Con datos parciales:** 81  
- **Sin datos encontrados:** 0  
- **Costo estimado por empresa Ãºtil:** 1.89 horas  

---

### Hallazgos clave

1. Datos incompletos en la pÃ¡gina principal  
2. Carga asincrÃ³nica de contenido  
3. URLs invÃ¡lidas o redireccionadas  
4. Datos mezclados o mal formateados  

---

## 14. Mejoras Sugeridas

- Ajustar patrones de `regex`  
- Filtrar URLs sin `http`  
- Documentar casos sin datos  
- Explorar subpÃ¡ginas automÃ¡ticamente  
- Usar `Selenium` o `Playwright`  
- Consultar APIs internas  
- Registrar logs por empresa  

---

## 15. ReflexiÃ³n Final

Este ejercicio permitiÃ³ identificar las limitaciones del scraping tradicional y la necesidad de
modularizar el flujo. Se documentaron errores como evidencia de aprendizaje y se dejaron mejoras
futuras para auditorÃ­a y enseÃ±anza.

---

## 16. Tiempo y Esfuerzo Invertido

**Tareas realizadas:**

- PreparaciÃ³n del entorno virtual  
- Limpieza y validaciÃ³n de nombres  
- GestiÃ³n de asincronÃ­a y subpÃ¡ginas  
- DocumentaciÃ³n de incidentes  
- ComparaciÃ³n de flujos  
- GeneraciÃ³n de evidencia  

**Tiempo total invertido:** ~36 horas distribuidas en 3 dÃ­as intensivos

---

## 17. Tabla de Incidentes TÃ©cnicos y Validaciones

| Tipo de incidente         | DescripciÃ³n breve                                      | Ejemplo o patrÃ³n detectado                          | Mejora futura sugerida                                 |
|---------------------------|--------------------------------------------------------|-----------------------------------------------------|--------------------------------------------------------|
| `#asincronÃ­a`             | Carga de datos depende de JavaScript o peticiones dinÃ¡micas | PÃ¡ginas sin datos en HTML estÃ¡tico              | Usar `Selenium` o capturas manuales                    |
| `#estructura_inaccesible` | Datos embebidos en imÃ¡genes o banners                  | Contacto solo en PDF o banners                      | Documentar como â€œno extraÃ­ble automÃ¡ticamenteâ€         |
| `#subpÃ¡gina_oculta`       | Datos en subpÃ¡ginas no enlazadas desde el home         | SecciÃ³n â€œContactoâ€ sin enlace directo               | Detectar enlaces internos relevantes                   |
| `#validaciÃ³n_manual`      | Requiere criterio humano para interpretar ambigÃ¼edades | Nombres genÃ©ricos o duplicados                      | Crear diccionario personalizado y registrar ejemplos   |
| `#datos_incompletos`      | Se extrae solo parte del contacto                      | Formularios sin email visible                       | Marcar como â€œparcialâ€ y registrar en log               |
| `#estructura_inconsistente`| HTML varÃ­a entre empresas, dificultando selectores Ãºnicos | Plantillas distintas o CMS personalizado         | Modularizar el scraping por tipo de estructura         |
| `#bloqueo_scraping`       | El sitio detecta scraping y bloquea o redirige         | Cloudflare, captcha o redirecciÃ³n forzada           | Documentar como â€œrequiere intervenciÃ³n manualâ€         |
| `#errores_de_entorno`     | Problemas con librerÃ­as, rutas o entornos virtuales    | Conflictos entre kernels o rutas relativas          | Registrar en README y limpiar entornos obsoletos       |


```

---

## Extra:
scraping-contactos-empresariales/
â”œâ”€â”€ README.md                  # Informe tÃ©cnico en formato Markdown
â”œâ”€â”€ requirements.txt           # LibrerÃ­as necesarias para reproducir el entorno
â”œâ”€â”€ .gitignore                 # Exclusiones bien justificadas
â”œâ”€â”€ /src/                      # Scripts del flujo tÃ©cnico
â”‚   â”œâ”€â”€ normalizacion.py
â”‚   â”œâ”€â”€ correccion.py
â”‚   â”œâ”€â”€ validacion.py
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ /data/                     # Archivos CSV de entrada y salida
â”‚   â”œâ”€â”€ empresas_originales.csv
â”‚   â”œâ”€â”€ empresas_limpias_corregidas_mejorado.csv
â”‚   â”œâ”€â”€ revision_manual.csv
â”‚   â”œâ”€â”€ correcciones_sospechosas.csv
â”‚   â”œâ”€â”€ empresas_limpias_corregidas_final.csv
â”‚   â””â”€â”€ log_de_correcciones.csv
â”œâ”€â”€ /docs/                     # DocumentaciÃ³n adicional y video explicativo
â”‚   â”œâ”€â”€ flujo_scraping.mp4
â”‚   â””â”€â”€ cobertura_scraping.md
â”œâ”€â”€ /notebooks/                # Jupyter opcional para exploraciÃ³n y validaciÃ³n
â”‚   â””â”€â”€ exploracion_contactos.ipynb
â””â”€â”€ /logs/                     # Logs funcionales por empresa
    â””â”€â”€ log_por_empresa.json

```

---
