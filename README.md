# Prueba TÃ©cnica: Scraping de InformaciÃ³n en PÃ¡gina Web

**ExtracciÃ³n automatizada de datos de contacto empresarial desde fuentes pÃºblicas**  
**Autora:** Amelia Cristina Herrera BriceÃ±o  
**Rol:** Data Analyst & Scientist en transiciÃ³n hacia BI y automatizaciÃ³n  
**Entorno de trabajo:** Python Â· Pandas Â· BeautifulSoup Â· Regex Â· CSV Â· ExploraciÃ³n reproducible  
**Fecha de entrega:** 15 Agosto 2025  

<p align="center"><img src="https://github.com/melinnicri/Proyecto-Nombres-Empresas/blob/main/image/scrapeo.png"></p>

---

## 1. Problema

Se cuenta con listados de empresas (nombre y CIF), pero se carece de sus datos de contacto directo 
(emails corporativos, telÃ©fonos, direcciones). Para establecer comunicaciÃ³n comercial, es necesario 
localizar y extraer automÃ¡ticamente esta informaciÃ³n desde sus sitios web oficiales u otras fuentes 
pÃºblicas disponibles en internet.

---

## 2. SoluciÃ³n TÃ©cnica

Desarrollar un proceso de scraping web que permita obtener datos de contacto empresariales de forma 
automatizada a partir de los nombres de las empresas.

---

## 3. Alcance de la Prueba

- **Dataset:** Muestra de 50â€“100 empresas espaÃ±olas (nombre + CIF)
- **Proceso esperado:**
  1. Localizar la web corporativa oficial de cada empresa
  2. Extraer datos de contacto mediante scraping web
- **Datos objetivo:**
  - URL oficial
  - Email corporativo principal (info@, contacto@, comercial@)
  - TelÃ©fono de contacto
  - DirecciÃ³n postal completa
- **Entregables:**
  - Archivo CSV/Excel con los datos obtenidos
  - DocumentaciÃ³n tÃ©cnica del proceso
  - Registro de limitaciones (robots.txt, captchas, etc.)
  - Tiempo estimado de procesamiento por empresa
  - Coste estimado del proceso
  - Video explicativo del flujo tÃ©cnico

---

## 4. ImplementaciÃ³n TÃ©cnica

- Scraping sincrÃ³nico reproducible: `requests` + `BeautifulSoup` + `regex`
- Limpieza post-scraping: DataFrames exportables, validaciÃ³n de encoding y estructura
- ExportaciÃ³n: Archivos CSV listos para anÃ¡lisis y visualizaciÃ³n

---

## 5. MÃ©tricas y EvaluaciÃ³n

- **Tasa de Ã©xito:** porcentaje de empresas con datos completos
- **Limitaciones encontradas:** robots.txt, captchas, pÃ¡ginas sin datos visibles
- **Tiempo estimado por empresa:** medido con barra de progreso
- **Coste estimado:** bajo en entorno local; escalable con APIs

---

## 6. Video Explicativo

ğŸ¥ Se graba un video mostrando el flujo, explicando el cÃ³digo y los resultados obtenidos.  
_Pendiente de ediciÃ³n y subida._

---

## 7. Escalabilidad

Para flujos masivos, se propone una versiÃ³n asincrÃ³nica con `aiohttp` o `crawl4ai`, que mejora el 
rendimiento y permite scraping concurrente.

---

## 8. Comparativa de MÃ©todos de BÃºsqueda de URLs Oficiales


| MÃ©todo                    | Gratuito | LÃ­mite         | Costo estimado | Ventajas                            | Desventajas                           |
|---------------------------|----------|----------------|----------------|-------------------------------------|---------------------------------------|
| googlesearch (scraping)   | SÃ­       | No oficial     | $0             | Sin registro, ideal para pruebas    | Bloqueos, resultados no estructurados |
| SerpAPI                   | SÃ­ (250) | Pago desde $50 | ~$50           | JSON limpio, sin bloqueo            | Requiere API Key, coste mensual       |
| Zenserp                   | SÃ­ (50)  | Pago desde $29 | ~$29           | FÃ¡cil de usar, resultados precisos  | Menor volumen gratuito                |
| Google Custom Search API  | SÃ­ (100) | $5/1000        | ~$5            | Oficial, configurable               | Requiere configuraciÃ³n previa         |
| Bing Search API           | SÃ­ (1000)| Pago desde $3  | ~$3            | EconÃ³mica, buena cobertura          | Menor precisiÃ³n en empresas locales   |

---

## 9. RecomendaciÃ³n Final

Para esta prueba tÃ©cnica (50â€“100 empresas), se recomienda utilizar `googlesearch` por su simplicidad 
y coste cero. Para escalar el proceso, se sugiere evaluar APIs comerciales como `SerpAPI` y adoptar 
scraping asincrÃ³nico con `aiohttp` o `crawl4ai`.

---

## 10. CorrecciÃ³n FonÃ©tica y SemÃ¡ntica de Nombres Empresariales

Objetivo: corregir nombres empresariales con validaciÃ³n fonÃ©tica (`rapidfuzz`) y semÃ¡ntica (`unidecode`), 
asegurando trazabilidad y limpieza formal.

### Ejemplo de correcciÃ³n fonÃ©tica

```python
import re
from unidecode import unidecode

def normalizar_nombre(nombre):
    nombre = unidecode(nombre)
    nombre = re.sub(r'[^\w\s]', '', nombre)
    nombre = nombre.strip().upper()
    return nombre

nombre_1 = "CompaÃ±Ã­a ABC S.A."
nombre_2 = "Compania - ABC, SA"

print(normalizar_nombre(nombre_1))  # COMPANIA ABC SA
print(normalizar_nombre(nombre_2))  # COMPANIA ABC SA
```

---

## 11. Flujo de CorrecciÃ³n y ValidaciÃ³n

Este mÃ³dulo se encarga de limpiar, corregir y validar los nombres empresariales antes del scraping,
asegurando trazabilidad y coherencia semÃ¡ntica.

| Archivo           | PropÃ³sito principal                                                         |
|-------------------|-----------------------------------------------------------------------------|
| `normalizacion.py`| Funciones para limpiar y normalizar nombres (acentos, sÃ­mbolos, mayÃºsculas) |
| `correccion.py`   | CorrecciÃ³n fonÃ©tica con `RapidFuzz` y validaciÃ³n semÃ¡ntica con `unidecode`  |
| `validacion.py`   | DetecciÃ³n de correcciones sospechosas y generaciÃ³n de revisiÃ³n manual       |
| `main.py`         | OrquestaciÃ³n del flujo completo: carga, limpieza, correcciÃ³n y exportaciÃ³n  |

> Cada paso del flujo estÃ¡ documentado y modularizado, permitiendo auditorÃ­a, mejora continua y enseÃ±anza tÃ©cnica.

---

## 12. Archivos CSV y Logs Funcionales

| Archivo CSV                                | Rol en el flujo                            | Contenido esperado                             | Sugerencias de mejora                              |
|--------------------------------------------|--------------------------------------------|------------------------------------------------|----------------------------------------------------|
| `empresas_limpias_corregidas_mejorado.csv` | Post-normalizaciÃ³n y correcciÃ³n automÃ¡tica | Nombres limpios + correcciones automÃ¡ticas     | Agregar columna `ORIGEN_CORRECCIÃ“N` y `FECHA_PROCESO`       |
| `revision_manual.csv`                      | Correcciones manuales aplicadas            | Casos lÃ­mite revisados por uno mismo           | Agregar columna `OBSERVACIONES` y `VALIDACIÃ“N_MANUAL`   |
| `correcciones_sospechosas.csv`             | Casos con ambigÃ¼edad o errores detectados  | Correcciones dudosas, posibles sobreajustes    | Agregar columna `TIPO_ERROR` y `RECOMENDACIÃ“N`                            |
| `empresas_limpias_corregidas_final.csv`    | Resultado final validado                   | Nombres corregidos y validados                 | Agregar columna `VALIDACIÃ“N_FINAL` y `FUENTE_CORRECCIÃ“N`   |
| `log_de_correcciones.csv`                  | Registro trazable de cada correcciÃ³n       | Entradas, salidas, tipo de correcciÃ³n          | Ya bien estructurado, solo falta `ID` y `TIMESTAMP`                       |

---

## 13. Informe de Cobertura y Limitaciones del Scraping

- **Empresas procesadas:** 100  
- **Con datos completos:** 19  
- **Con datos parciales:** 81  
- **Sin datos encontrados:** 0  
- **Costo estimado por empresa Ãºtil:** 1.89 horas  

---

### Hallazgos clave

1. Datos incompletos en la pÃ¡gina principal  
2. Carga asincrÃ³nica de contenido  
3. URLs invÃ¡lidas o redireccionadas  
4. Datos mezclados o mal formateados  

---

## 14. Mejoras Sugeridas

- Ajustar patrones de `regex`  
- Filtrar URLs sin `http`  
- Documentar casos sin datos  
- Explorar subpÃ¡ginas automÃ¡ticamente  
- Usar `Selenium` o `Playwright`  
- Consultar APIs internas  
- Registrar logs por empresa  

---

## 15. ReflexiÃ³n Final

Este ejercicio permitiÃ³ identificar las limitaciones del scraping tradicional y la necesidad de
modularizar el flujo. Se documentaron errores como evidencia de aprendizaje y se dejaron mejoras
futuras para auditorÃ­a y enseÃ±anza.

---

## 16. Tiempo y Esfuerzo Invertido

**Tareas realizadas:**

- PreparaciÃ³n del entorno virtual  
- Limpieza y validaciÃ³n de nombres  
- GestiÃ³n de asincronÃ­a y subpÃ¡ginas  
- DocumentaciÃ³n de incidentes  
- ComparaciÃ³n de flujos  
- GeneraciÃ³n de evidencia  

**Tiempo total invertido:** ~36 horas distribuidas en 3 dÃ­as intensivos

---

## 17. Tabla de Incidentes TÃ©cnicos y Validaciones

| Tipo de incidente         | DescripciÃ³n breve                                      | Ejemplo o patrÃ³n detectado                          | Mejora futura sugerida                                 |
|---------------------------|--------------------------------------------------------|-----------------------------------------------------|--------------------------------------------------------|
| `#asincronÃ­a`             | Carga de datos depende de JavaScript o peticiones dinÃ¡micas | PÃ¡ginas sin datos en HTML estÃ¡tico              | Usar `Selenium` o capturas manuales                    |
| `#estructura_inaccesible` | Datos embebidos en imÃ¡genes o banners                  | Contacto solo en PDF o banners                      | Documentar como â€œno extraÃ­ble automÃ¡ticamenteâ€         |
| `#subpÃ¡gina_oculta`       | Datos en subpÃ¡ginas no enlazadas desde el home         | SecciÃ³n â€œContactoâ€ sin enlace directo               | Detectar enlaces internos relevantes                   |
| `#validaciÃ³n_manual`      | Requiere criterio humano para interpretar ambigÃ¼edades | Nombres genÃ©ricos o duplicados                      | Crear diccionario personalizado y registrar ejemplos   |
| `#datos_incompletos`      | Se extrae solo parte del contacto                      | Formularios sin email visible                       | Marcar como â€œparcialâ€ y registrar en log               |
| `#estructura_inconsistente`| HTML varÃ­a entre empresas, dificultando selectores Ãºnicos | Plantillas distintas o CMS personalizado         | Modularizar el scraping por tipo de estructura         |
| `#bloqueo_scraping`       | El sitio detecta scraping y bloquea o redirige         | Cloudflare, captcha o redirecciÃ³n forzada           | Documentar como â€œrequiere intervenciÃ³n manualâ€         |
| `#errores_de_entorno`     | Problemas con librerÃ­as, rutas o entornos virtuales    | Conflictos entre kernels o rutas relativas          | Registrar en README y limpiar entornos obsoletos       |

---

## Extras:

---

```

# Diagrama del proyecto:
Proyecto-Nombres-Empresas/
â”œâ”€â”€ image/
â”‚   â””â”€â”€ scrapeo/
â”œâ”€â”€ mi_proyecto_escrapeo/
â”‚   â”œâ”€â”€ escrapeo/
â”‚   â”‚   â”œâ”€â”€ data/                                                    # Archivos CSV de entrada y salida
â”‚   â”‚   â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ contacto_empresas_es.csv
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ empresas_completas.csv
â”‚   â”‚   â”‚   â””â”€â”€ raw/
â”‚   â”‚   â”‚       â””â”€â”€ comparados.csv
â”‚   â”‚   â”œâ”€â”€ src/                                                      # Scripts del flujo tÃ©cnico
â”‚   â”‚   â”‚   â””â”€â”€ comparativa.ipynb
â”‚   â”‚   â”œâ”€â”€ escrapeo_1.py
â”‚   â”‚   â””â”€â”€ requirements.txt                                          # LibrerÃ­as necesarias para reproducir el entorno para escrapeo
â”‚   â””â”€â”€ proyecto_correccion/
â”‚       â”œâ”€â”€ __pycache__/
â”‚       â”‚   â”œâ”€â”€ correccion.cpython-312.pyc
â”‚       â”‚   â”œâ”€â”€ normalizacion.cpython-312.pyc
â”‚       â”‚   â””â”€â”€ validacion.cpython-312.pyc
â”‚       â”œâ”€â”€ data/                                                      # Archivos CSV de entrada y salida
â”‚       â”‚   â”œâ”€â”€ processed/
â”‚       â”‚   â”‚   â”œâ”€â”€ empresas_limpias_corregidas_final.csv
â”‚       â”‚   â”‚   â””â”€â”€ empresas_limpias_corregidas_mejorado.csv
â”‚       â”‚   â””â”€â”€ raw/
â”‚       â”‚       â””â”€â”€ 100empresas.csv
â”‚       â”œâ”€â”€ logs/                                                       # Logs funcionales de la correciÃ³n de los nombres de empresas
â”‚       â”‚   â”œâ”€â”€ correcciones_sospechosas.csv
â”‚       â”‚   â”œâ”€â”€ log_de_correcciones.csv
â”‚       â”‚   â””â”€â”€ revision_manual.csv
â”‚       â”œâ”€â”€ src/                                                        # Scripts del flujo tÃ©cnico
â”‚       â”‚   â”œâ”€â”€ modules/
â”‚       â”‚   â”‚   â”œâ”€â”€ correccion.py
â”‚       â”‚   â”‚   â”œâ”€â”€ normalizacion.py
â”‚       â”‚   â”‚   â””â”€â”€ validacion.py
â”‚       â”‚   â””â”€â”€ main.py
â”‚       â””â”€â”€ requirements.txt                                            # LibrerÃ­as necesarias para reproducir el entorno para correcciÃ³n
â”œâ”€â”€ README.md                                                           # Informe tÃ©cnico en formato Markdown


```

---

```
## Diagrama de flujo funcional del proyecto

Inicio
  â†“
ğŸ“ Carga de datos iniciales (100empresas.csv)
  â†“
ğŸ§¹ CorrecciÃ³n fonÃ©tica y semÃ¡ntica
  â”œâ”€> correccion.py
  â”œâ”€> normalizacion.py
  â””â”€> validacion.py
  â†“
ğŸ“Š GeneraciÃ³n de empresas_limpias_corregidas_final.csv
  â†“
ğŸ“ Registro de correcciones en logs/*.csv
  â†“
ğŸŒ Scrapeo de nombres corregidos
  â”œâ”€> escrapeo_1.py
  â””â”€> comparativa.ipynb
  â†“
ğŸ“ Almacenamiento en contacto_empresas_es.csv y empresas_completas.csv
  â†“
ğŸ“š DocumentaciÃ³n en README.md
  â†“
Fin

```
---



```
## Diagrama de la carpeta "crawler" (escrapeo hÃ­brido, asincrÃ³nico, con fallback dinÃ¡mico y validadciÃ³n semÃ¡ntica, realizado con Crawl4ai).

crawler/
â”œâ”€â”€ __pycache__/                     # Archivos compilados automÃ¡ticamente por Python
â”‚   â”œâ”€â”€ contact_extractor.cpython-313.pyc
â”‚   â”œâ”€â”€ correccion_crawler.cpython-313.pyc
â”‚   â”œâ”€â”€ diccionario_manual.cpython-313.pyc
â”‚   â”œâ”€â”€ gestor_correcciones.cpython-313.pyc
â”‚   â”œâ”€â”€ normalizacion_crawler.cpython-313.pyc
â”‚   â”œâ”€â”€ scraping_utils.cpython-313.pyc
â”‚   â””â”€â”€ validacion_crawler.cpython-313.pyc

â”œâ”€â”€ data/                            # Ecosistema de datos
â”‚   â”œâ”€â”€ raw/                         # Datos originales sin procesar
â”‚   â”‚   â””â”€â”€ 100empresas.csv
â”‚   â”œâ”€â”€ processed/                   # Datos corregidos, validados y listos para anÃ¡lisis
â”‚   â”‚   â”œâ”€â”€ contacto_empresas_es_2.csv*
â”‚   â”‚   â”œâ”€â”€ correcciones_sospechosas.csv
â”‚   â”‚   â”œâ”€â”€ diccionario_nombres_corregidos.csv
â”‚   â”‚   â”œâ”€â”€ empresas_limpias_corregidas_final.csv
â”‚   â”‚   â”œâ”€â”€ empresas_limpias_corregidas_mejorado.csv
â”‚   â”‚   â”œâ”€â”€ nombres_normalizados_para_scraping.csv
â”‚   â”‚   â””â”€â”€ revision_manual.csv
â”‚   â””â”€â”€ diccionario_manual.py        # Correcciones manuales y mapeos heurÃ­sticos

â”œâ”€â”€ logs/                            # AuditorÃ­a del proceso y scripts auxiliares
â”‚   â”œâ”€â”€ auditoria_texto_extraido.py
â”‚   â”œâ”€â”€ contact_extractor.py
â”‚   â”œâ”€â”€ gestor_correcciones.py
â”‚   â”œâ”€â”€ log_de_correcciones.csv
â”‚   â”œâ”€â”€ main_async.py
â”‚   â””â”€â”€ scraping_utils.py

â”œâ”€â”€ src/                             # CÃ³digo principal y mÃ³dulos reutilizables
â”‚   â”œâ”€â”€ modules/                     # Funciones compartidas y componentes tÃ©cnicos
â”‚   â””â”€â”€ main.py                      # Script principal de ejecuciÃ³n

â”œâ”€â”€ compar2.ipynb                    # Notebook de comparaciÃ³n o anÃ¡lisis exploratorio
â”œâ”€â”€ escrapeo_dos.py                  # Script de scraping hÃ­brido con fallback dinÃ¡mico
â””â”€â”€ requirements.txt                 # Lista de dependencias del proyecto

Resultado: extrae los urls con pocos errores, pero direcciÃ³n, telÃ©fono y email no los alcanza a extraer. En 36 segundos.
```


```
AgreguÃ© otra versiÃ³n mejorada del sistema asÃ­ncrona de scraping de informaciÃ³n de empresas espaÃ±olas: version_02
VersiÃ³n_02/
â”œâ”€â”€ nombres_scraping/                  # Punto de partida del proceso
â”‚   â””â”€â”€ buscar_url.py                  # Sistema asincrÃ³nico que busca URLs
â”‚   â””â”€â”€ urls.csv                       # Resultado exitoso (Â¡por fin!)
â”‚
â”œâ”€â”€ mÃ³dulos_contacto/                 # ExtracciÃ³n y validaciÃ³n de contacto
â”‚   â””â”€â”€ extractor.py                  # Auxiliar para extracciÃ³n
â”‚   â””â”€â”€ pipeline_contacto.py          # Pipeline principal
â”‚   â””â”€â”€ contacto.csv                  # Resultado con datos de contacto
â”‚
â”œâ”€â”€ anÃ¡lisis/                         # EstadÃ­sticas y validaciones
â”‚   â””â”€â”€ estadÃ­stica.ipynb             # Notebook con anÃ¡lisis de urls y contacto
â”œâ”€â”€ requirements.txt                  # Requerimientos del entorno virtual para trabajarlo
```
