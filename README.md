# Prueba T√©cnica: Scraping de Informaci√≥n en P√°gina Web

**Extracci√≥n automatizada de datos de contacto empresarial desde fuentes p√∫blicas**  
**Autora:** Amelia Cristina Herrera Brice√±o  
**Rol:** Data Analyst & Scientist en transici√≥n hacia BI y automatizaci√≥n  
**Entorno de trabajo:** Python ¬∑ Pandas ¬∑ BeautifulSoup ¬∑ Regex ¬∑ CSV ¬∑ Exploraci√≥n reproducible  
**Fecha de entrega:** 15 Agosto 2025  

<p align="center"><img src="https://github.com/melinnicri/Proyecto-Nombres-Empresas/blob/main/image/scrapeo.png"></p>

---

## 1. Problema

Se cuenta con listados de empresas (nombre y CIF), pero se carece de sus datos de contacto directo 
(emails corporativos, tel√©fonos, direcciones). Para establecer comunicaci√≥n comercial, es necesario 
localizar y extraer autom√°ticamente esta informaci√≥n desde sus sitios web oficiales u otras fuentes 
p√∫blicas disponibles en internet.

---

## 2. Soluci√≥n T√©cnica

Desarrollar un proceso de scraping web que permita obtener datos de contacto empresariales de forma 
automatizada a partir de los nombres de las empresas.

---

## 3. Alcance de la Prueba

- **Dataset:** Muestra de 50‚Äì100 empresas espa√±olas (nombre + CIF)
- **Proceso esperado:**
  1. Localizar la web corporativa oficial de cada empresa
  2. Extraer datos de contacto mediante scraping web
- **Datos objetivo:**
  - URL oficial
  - Email corporativo principal (info@, contacto@, comercial@)
  - Tel√©fono de contacto
  - Direcci√≥n postal completa
- **Entregables:**
  - Archivo CSV/Excel con los datos obtenidos
  - Documentaci√≥n t√©cnica del proceso
  - Registro de limitaciones (robots.txt, captchas, etc.)
  - Tiempo estimado de procesamiento por empresa
  - Coste estimado del proceso
  - Video explicativo del flujo t√©cnico

---

## 4. Implementaci√≥n T√©cnica

- Scraping sincr√≥nico reproducible: `requests` + `BeautifulSoup` + `regex`
- Limpieza post-scraping: DataFrames exportables, validaci√≥n de encoding y estructura
- Exportaci√≥n: Archivos CSV listos para an√°lisis y visualizaci√≥n

---

## 5. M√©tricas y Evaluaci√≥n

- **Tasa de √©xito:** porcentaje de empresas con datos completos
- **Limitaciones encontradas:** robots.txt, captchas, p√°ginas sin datos visibles
- **Tiempo estimado por empresa:** medido con barra de progreso
- **Coste estimado:** bajo en entorno local; escalable con APIs

---

## 6. Video Explicativo

üé• Se graba un video mostrando el flujo, explicando el c√≥digo y los resultados obtenidos.  
_Pendiente de edici√≥n y subida._

---

## 7. Escalabilidad

Para flujos masivos, se propone una versi√≥n asincr√≥nica con `aiohttp` o `crawl4ai`, que mejora el 
rendimiento y permite scraping concurrente.

---

## 8. Comparativa de M√©todos de B√∫squeda de URLs Oficiales


| M√©todo                    | Gratuito | L√≠mite         | Costo estimado | Ventajas                            | Desventajas                           |
|---------------------------|----------|----------------|----------------|-------------------------------------|---------------------------------------|
| googlesearch (scraping)   | S√≠       | No oficial     | $0             | Sin registro, ideal para pruebas    | Bloqueos, resultados no estructurados |
| SerpAPI                   | S√≠ (250) | Pago desde $50 | ~$50           | JSON limpio, sin bloqueo            | Requiere API Key, coste mensual       |
| Zenserp                   | S√≠ (50)  | Pago desde $29 | ~$29           | F√°cil de usar, resultados precisos  | Menor volumen gratuito                |
| Google Custom Search API  | S√≠ (100) | $5/1000        | ~$5            | Oficial, configurable               | Requiere configuraci√≥n previa         |
| Bing Search API           | S√≠ (1000)| Pago desde $3  | ~$3            | Econ√≥mica, buena cobertura          | Menor precisi√≥n en empresas locales   |

---

## 9. Recomendaci√≥n Final

Para esta prueba t√©cnica (50‚Äì100 empresas), se recomienda utilizar `googlesearch` por su simplicidad 
y coste cero. Para escalar el proceso, se sugiere evaluar APIs comerciales como `SerpAPI` y adoptar 
scraping asincr√≥nico con `aiohttp` o `crawl4ai`.

---

## 10. Correcci√≥n Fon√©tica y Sem√°ntica de Nombres Empresariales

Objetivo: corregir nombres empresariales con validaci√≥n fon√©tica (`rapidfuzz`) y sem√°ntica (`unidecode`), 
asegurando trazabilidad y limpieza formal.

### Ejemplo de correcci√≥n fon√©tica

```python
import re
from unidecode import unidecode

def normalizar_nombre(nombre):
    nombre = unidecode(nombre)
    nombre = re.sub(r'[^\w\s]', '', nombre)
    nombre = nombre.strip().upper()
    return nombre

nombre_1 = "Compa√±√≠a ABC S.A."
nombre_2 = "Compania - ABC, SA"

print(normalizar_nombre(nombre_1))  # COMPANIA ABC SA
print(normalizar_nombre(nombre_2))  # COMPANIA ABC SA
```

---

## 11. Flujo de Correcci√≥n y Validaci√≥n

Este m√≥dulo se encarga de limpiar, corregir y validar los nombres empresariales antes del scraping,
asegurando trazabilidad y coherencia sem√°ntica.

| Archivo           | Prop√≥sito principal                                                         |
|-------------------|-----------------------------------------------------------------------------|
| `normalizacion.py`| Funciones para limpiar y normalizar nombres (acentos, s√≠mbolos, may√∫sculas) |
| `correccion.py`   | Correcci√≥n fon√©tica con `RapidFuzz` y validaci√≥n sem√°ntica con `unidecode`  |
| `validacion.py`   | Detecci√≥n de correcciones sospechosas y generaci√≥n de revisi√≥n manual       |
| `main.py`         | Orquestaci√≥n del flujo completo: carga, limpieza, correcci√≥n y exportaci√≥n  |

> Cada paso del flujo est√° documentado y modularizado, permitiendo auditor√≠a, mejora continua y ense√±anza t√©cnica.

---

## 12. Archivos CSV y Logs Funcionales

| Archivo CSV                                | Rol en el flujo                            | Contenido esperado                             | Sugerencias de mejora                              |
|--------------------------------------------|--------------------------------------------|------------------------------------------------|----------------------------------------------------|
| `empresas_limpias_corregidas_mejorado.csv` | Post-normalizaci√≥n y correcci√≥n autom√°tica | Nombres limpios + correcciones autom√°ticas     | Agregar columna `ORIGEN_CORRECCI√ìN` y `FECHA_PROCESO`       |
| `revision_manual.csv`                      | Correcciones manuales aplicadas            | Casos l√≠mite revisados por uno mismo           | Agregar columna `OBSERVACIONES` y `VALIDACI√ìN_MANUAL`   |
| `correcciones_sospechosas.csv`             | Casos con ambig√ºedad o errores detectados  | Correcciones dudosas, posibles sobreajustes    | Agregar columna `TIPO_ERROR` y `RECOMENDACI√ìN`                            |
| `empresas_limpias_corregidas_final.csv`    | Resultado final validado                   | Nombres corregidos y validados                 | Agregar columna `VALIDACI√ìN_FINAL` y `FUENTE_CORRECCI√ìN`   |
| `log_de_correcciones.csv`                  | Registro trazable de cada correcci√≥n       | Entradas, salidas, tipo de correcci√≥n          | Ya bien estructurado, solo falta `ID` y `TIMESTAMP`                       |

---

## 13. Informe de Cobertura y Limitaciones del Scraping

- **Empresas procesadas:** 100  
- **Con datos completos:** 19  
- **Con datos parciales:** 81  
- **Sin datos encontrados:** 0  
- **Costo estimado por empresa √∫til:** 1.89 horas  

---

### Hallazgos clave

1. Datos incompletos en la p√°gina principal  
2. Carga asincr√≥nica de contenido  
3. URLs inv√°lidas o redireccionadas  
4. Datos mezclados o mal formateados  

---

## 14. Mejoras Sugeridas

- Ajustar patrones de `regex`  
- Filtrar URLs sin `http`  
- Documentar casos sin datos  
- Explorar subp√°ginas autom√°ticamente  
- Usar `Selenium` o `Playwright`  
- Consultar APIs internas  
- Registrar logs por empresa  

---

## 15. Reflexi√≥n Final

Este ejercicio permiti√≥ identificar las limitaciones del scraping tradicional y la necesidad de
modularizar el flujo. Se documentaron errores como evidencia de aprendizaje y se dejaron mejoras
futuras para auditor√≠a y ense√±anza.

---

## 16. Tiempo y Esfuerzo Invertido

**Tareas realizadas:**

- Preparaci√≥n del entorno virtual  
- Limpieza y validaci√≥n de nombres  
- Gesti√≥n de asincron√≠a y subp√°ginas  
- Documentaci√≥n de incidentes  
- Comparaci√≥n de flujos  
- Generaci√≥n de evidencia  

**Tiempo total invertido:** ~36 horas distribuidas en 3 d√≠as intensivos

---

## 17. Tabla de Incidentes T√©cnicos y Validaciones

| Tipo de incidente         | Descripci√≥n breve                                      | Ejemplo o patr√≥n detectado                          | Mejora futura sugerida                                 |
|---------------------------|--------------------------------------------------------|-----------------------------------------------------|--------------------------------------------------------|
| `#asincron√≠a`             | Carga de datos depende de JavaScript o peticiones din√°micas | P√°ginas sin datos en HTML est√°tico              | Usar `Selenium` o capturas manuales                    |
| `#estructura_inaccesible` | Datos embebidos en im√°genes o banners                  | Contacto solo en PDF o banners                      | Documentar como ‚Äúno extra√≠ble autom√°ticamente‚Äù         |
| `#subp√°gina_oculta`       | Datos en subp√°ginas no enlazadas desde el home         | Secci√≥n ‚ÄúContacto‚Äù sin enlace directo               | Detectar enlaces internos relevantes                   |
| `#validaci√≥n_manual`      | Requiere criterio humano para interpretar ambig√ºedades | Nombres gen√©ricos o duplicados                      | Crear diccionario personalizado y registrar ejemplos   |
| `#datos_incompletos`      | Se extrae solo parte del contacto                      | Formularios sin email visible                       | Marcar como ‚Äúparcial‚Äù y registrar en log               |
| `#estructura_inconsistente`| HTML var√≠a entre empresas, dificultando selectores √∫nicos | Plantillas distintas o CMS personalizado         | Modularizar el scraping por tipo de estructura         |
| `#bloqueo_scraping`       | El sitio detecta scraping y bloquea o redirige         | Cloudflare, captcha o redirecci√≥n forzada           | Documentar como ‚Äúrequiere intervenci√≥n manual‚Äù         |
| `#errores_de_entorno`     | Problemas con librer√≠as, rutas o entornos virtuales    | Conflictos entre kernels o rutas relativas          | Registrar en README y limpiar entornos obsoletos       |


```

---

## Extra:
scraping-contactos-empresariales/
‚îú‚îÄ‚îÄ README.md                  # Informe t√©cnico en formato Markdown
‚îú‚îÄ‚îÄ requirements.txt           # Librer√≠as necesarias para reproducir el entorno
‚îú‚îÄ‚îÄ .gitignore                 # Exclusiones bien justificadas
‚îú‚îÄ‚îÄ /src/                      # Scripts del flujo t√©cnico
‚îÇ   ‚îú‚îÄ‚îÄ normalizacion.py
‚îÇ   ‚îú‚îÄ‚îÄ correccion.py
‚îÇ   ‚îú‚îÄ‚îÄ validacion.py
‚îÇ   ‚îî‚îÄ‚îÄ main.py
‚îú‚îÄ‚îÄ /data/                     # Archivos CSV de entrada y salida
‚îÇ   ‚îú‚îÄ‚îÄ empresas_originales.csv
‚îÇ   ‚îú‚îÄ‚îÄ empresas_limpias_corregidas_mejorado.csv
‚îÇ   ‚îú‚îÄ‚îÄ revision_manual.csv
‚îÇ   ‚îú‚îÄ‚îÄ correcciones_sospechosas.csv
‚îÇ   ‚îú‚îÄ‚îÄ empresas_limpias_corregidas_final.csv
‚îÇ   ‚îî‚îÄ‚îÄ log_de_correcciones.csv
‚îú‚îÄ‚îÄ /docs/                     # Documentaci√≥n adicional y video explicativo
‚îÇ   ‚îú‚îÄ‚îÄ flujo_scraping.mp4
‚îÇ   ‚îî‚îÄ‚îÄ cobertura_scraping.md
‚îú‚îÄ‚îÄ /notebooks/                # Jupyter opcional para exploraci√≥n y validaci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ exploracion_contactos.ipynb
‚îî‚îÄ‚îÄ /logs/                     # Logs funcionales por empresa
    ‚îî‚îÄ‚îÄ log_por_empresa.json

```

---
