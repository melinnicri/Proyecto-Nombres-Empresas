# Prueba TÃ©cnica: Scraping de InformaciÃ³n en PÃ¡gina Web

**ExtracciÃ³n automatizada de datos de contacto empresarial desde fuentes pÃºblicas**  
**Autora:** Amelia Cristina Herrera BriceÃ±o  
**Rol:** Data Analyst & Scientist en transiciÃ³n hacia BI y automatizaciÃ³n  
**Entorno de trabajo:** Python Â· Pandas Â· BeautifulSoup Â· Regex Â· CSV Â· ExploraciÃ³n reproducible  
**Fecha de entrega:** 15 Agosto 2025  

<p align="center"><img src="https://github.com/melinnicri/Proyecto-Nombres-Empresas/blob/main/image/scrapeo.png"></p>

---

## 1. Problema

Se cuenta con listados de empresas (nombre y CIF), pero se carece de sus datos de contacto directo 
(emails corporativos, telÃ©fonos, direcciones). Para establecer comunicaciÃ³n comercial, es necesario 
localizar y extraer automÃ¡ticamente esta informaciÃ³n desde sus sitios web oficiales u otras fuentes 
pÃºblicas disponibles en internet.

---

## 2. SoluciÃ³n TÃ©cnica

Desarrollar un proceso de scraping web que permita obtener datos de contacto empresariales de forma 
automatizada a partir de los nombres de las empresas.

---

## 3. Alcance de la Prueba

- **Dataset:** Muestra de 50â€“100 empresas espaÃ±olas (nombre + CIF)
- **Proceso esperado:**
  1. Localizar la web corporativa oficial de cada empresa
  2. Extraer datos de contacto mediante scraping web
- **Datos objetivo:**
  - URL oficial
  - Email corporativo principal (info@, contacto@, comercial@)
  - TelÃ©fono de contacto
  - DirecciÃ³n postal completa
- **Entregables:**
  - Archivo CSV/Excel con los datos obtenidos
  - DocumentaciÃ³n tÃ©cnica del proceso
  - Registro de limitaciones (robots.txt, captchas, etc.)
  - Tiempo estimado de procesamiento por empresa
  - Coste estimado del proceso
  - Video explicativo del flujo tÃ©cnico

---

## 4. ImplementaciÃ³n TÃ©cnica

- Scraping sincrÃ³nico reproducible: `requests` + `BeautifulSoup` + `regex`
- Limpieza post-scraping: DataFrames exportables, validaciÃ³n de encoding y estructura
- ExportaciÃ³n: Archivos CSV listos para anÃ¡lisis y visualizaciÃ³n

---

## 5. MÃ©tricas y EvaluaciÃ³n

- **Tasa de Ã©xito:** porcentaje de empresas con datos completos
- **Limitaciones encontradas:** robots.txt, captchas, pÃ¡ginas sin datos visibles
- **Tiempo estimado por empresa:** medido con barra de progreso
- **Coste estimado:** bajo en entorno local; escalable con APIs

---

## 6. Video Explicativo

ğŸ¥ Se graba un video mostrando el flujo, explicando el cÃ³digo y los resultados obtenidos.  
_Pendiente de ediciÃ³n y subida._

---

## 7. Escalabilidad

Para flujos masivos, se propone una versiÃ³n asincrÃ³nica con `aiohttp` o `crawl4ai`, que mejora el 
rendimiento y permite scraping concurrente.

---

## 8. Comparativa de MÃ©todos de BÃºsqueda de URLs Oficiales


| MÃ©todo                    | Gratuito | LÃ­mite         | Costo estimado | Ventajas                            | Desventajas                           |
|---------------------------|----------|----------------|----------------|-------------------------------------|---------------------------------------|
| googlesearch (scraping)   | SÃ­       | No oficial     | $0             | Sin registro, ideal para pruebas    | Bloqueos, resultados no estructurados |
| SerpAPI                   | SÃ­ (250) | Pago desde $50 | ~$50           | JSON limpio, sin bloqueo            | Requiere API Key, coste mensual       |
| Zenserp                   | SÃ­ (50)  | Pago desde $29 | ~$29           | FÃ¡cil de usar, resultados precisos  | Menor volumen gratuito                |
| Google Custom Search API  | SÃ­ (100) | $5/1000        | ~$5            | Oficial, configurable               | Requiere configuraciÃ³n previa         |
| Bing Search API           | SÃ­ (1000)| Pago desde $3  | ~$3            | EconÃ³mica, buena cobertura          | Menor precisiÃ³n en empresas locales   |

---

## 9. RecomendaciÃ³n Final

Para esta prueba tÃ©cnica (50â€“100 empresas), se recomienda utilizar `googlesearch` por su simplicidad 
y coste cero. Para escalar el proceso, se sugiere evaluar APIs comerciales como `SerpAPI` y adoptar 
scraping asincrÃ³nico con `aiohttp` o `crawl4ai`.

---

## 10. CorrecciÃ³n FonÃ©tica y SemÃ¡ntica de Nombres Empresariales

Objetivo: corregir nombres empresariales con validaciÃ³n fonÃ©tica (`rapidfuzz`) y semÃ¡ntica (`unidecode`), 
asegurando trazabilidad y limpieza formal.

### Ejemplo de correcciÃ³n fonÃ©tica

```python
import re
from unidecode import unidecode

def normalizar_nombre(nombre):
    nombre = unidecode(nombre)
    nombre = re.sub(r'[^\w\s]', '', nombre)
    nombre = nombre.strip().upper()
    return nombre

nombre_1 = "CompaÃ±Ã­a ABC S.A."
nombre_2 = "Compania - ABC, SA"

print(normalizar_nombre(nombre_1))  # COMPANIA ABC SA
print(normalizar_nombre(nombre_2))  # COMPANIA ABC SA
```

---

## 11. Flujo de CorrecciÃ³n y ValidaciÃ³n

Este mÃ³dulo se encarga de limpiar, corregir y validar los nombres empresariales antes del scraping,
asegurando trazabilidad y coherencia semÃ¡ntica.

| Archivo           | PropÃ³sito principal                                                         |
|-------------------|-----------------------------------------------------------------------------|
| `normalizacion.py`| Funciones para limpiar y normalizar nombres (acentos, sÃ­mbolos, mayÃºsculas) |
| `correccion.py`   | CorrecciÃ³n fonÃ©tica con `RapidFuzz` y validaciÃ³n semÃ¡ntica con `unidecode`  |
| `validacion.py`   | DetecciÃ³n de correcciones sospechosas y generaciÃ³n de revisiÃ³n manual       |
| `main.py`         | OrquestaciÃ³n del flujo completo: carga, limpieza, correcciÃ³n y exportaciÃ³n  |

> Cada paso del flujo estÃ¡ documentado y modularizado, permitiendo auditorÃ­a, mejora continua y enseÃ±anza tÃ©cnica.

---

## 12. Archivos CSV y Logs Funcionales

| Archivo CSV                                | Rol en el flujo                            | Contenido esperado                             | Sugerencias de mejora                              |
|--------------------------------------------|--------------------------------------------|------------------------------------------------|----------------------------------------------------|
| `empresas_limpias_corregidas_mejorado.csv` | Post-normalizaciÃ³n y correcciÃ³n automÃ¡tica | Nombres limpios + correcciones automÃ¡ticas     | Agregar columna `ORIGEN_CORRECCIÃ“N` y `FECHA_PROCESO`       |
| `revision_manual.csv`                      | Correcciones manuales aplicadas            | Casos lÃ­mite revisados por uno mismo           | Agregar columna `OBSERVACIONES` y `VALIDACIÃ“N_MANUAL`   |
| `correcciones_sospechosas.csv`             | Casos con ambigÃ¼edad o errores detectados  | Correcciones dudosas, posibles sobreajustes    | Agregar columna `TIPO_ERROR` y `RECOMENDACIÃ“N`                            |
| `empresas_limpias_corregidas_final.csv`    | Resultado final validado                   | Nombres corregidos y validados                 | Agregar columna `VALIDACIÃ“N_FINAL` y `FUENTE_CORRECCIÃ“N`   |
| `log_de_correcciones.csv`                  | Registro trazable de cada correcciÃ³n       | Entradas, salidas, tipo de correcciÃ³n          | Ya bien estructurado, solo falta `ID` y `TIMESTAMP`                       |

---

## 13. Informe de Cobertura y Limitaciones del Scraping

- **Empresas procesadas:** 100  
- **Con datos completos:** 19  
- **Con datos parciales:** 81  
- **Sin datos encontrados:** 0  
- **Costo estimado por empresa Ãºtil:** 1.89 horas  

---

### Hallazgos clave

1. Datos incompletos en la pÃ¡gina principal  
2. Carga asincrÃ³nica de contenido  
3. URLs invÃ¡lidas o redireccionadas  
4. Datos mezclados o mal formateados  

---

## 14. Mejoras Sugeridas

- Ajustar patrones de `regex`  
- Filtrar URLs sin `http`  
- Documentar casos sin datos  
- Explorar subpÃ¡ginas automÃ¡ticamente  
- Usar `Selenium` o `Playwright`  
- Consultar APIs internas  
- Registrar logs por empresa  

---

## 15. ReflexiÃ³n Final

Este ejercicio permitiÃ³ identificar las limitaciones del scraping tradicional y la necesidad de
modularizar el flujo. Se documentaron errores como evidencia de aprendizaje y se dejaron mejoras
futuras para auditorÃ­a y enseÃ±anza.

---

## 16. Tiempo y Esfuerzo Invertido

**Tareas realizadas:**

- PreparaciÃ³n del entorno virtual  
- Limpieza y validaciÃ³n de nombres  
- GestiÃ³n de asincronÃ­a y subpÃ¡ginas  
- DocumentaciÃ³n de incidentes  
- ComparaciÃ³n de flujos  
- GeneraciÃ³n de evidencia  

**Tiempo total invertido:** ~36 horas distribuidas en 3 dÃ­as intensivos

---

## 17. Tabla de Incidentes TÃ©cnicos y Validaciones

| Tipo de incidente         | DescripciÃ³n breve                                      | Ejemplo o patrÃ³n detectado                          | Mejora futura sugerida                                 |
|---------------------------|--------------------------------------------------------|-----------------------------------------------------|--------------------------------------------------------|
| `#asincronÃ­a`             | Carga de datos depende de JavaScript o peticiones dinÃ¡micas | PÃ¡ginas sin datos en HTML estÃ¡tico              | Usar `Selenium` o capturas manuales                    |
| `#estructura_inaccesible` | Datos embebidos en imÃ¡genes o banners                  | Contacto solo en PDF o banners                      | Documentar como â€œno extraÃ­ble automÃ¡ticamenteâ€         |
| `#subpÃ¡gina_oculta`       | Datos en subpÃ¡ginas no enlazadas desde el home         | SecciÃ³n â€œContactoâ€ sin enlace directo               | Detectar enlaces internos relevantes                   |
| `#validaciÃ³n_manual`      | Requiere criterio humano para interpretar ambigÃ¼edades | Nombres genÃ©ricos o duplicados                      | Crear diccionario personalizado y registrar ejemplos   |
| `#datos_incompletos`      | Se extrae solo parte del contacto                      | Formularios sin email visible                       | Marcar como â€œparcialâ€ y registrar en log               |
| `#estructura_inconsistente`| HTML varÃ­a entre empresas, dificultando selectores Ãºnicos | Plantillas distintas o CMS personalizado         | Modularizar el scraping por tipo de estructura         |
| `#bloqueo_scraping`       | El sitio detecta scraping y bloquea o redirige         | Cloudflare, captcha o redirecciÃ³n forzada           | Documentar como â€œrequiere intervenciÃ³n manualâ€         |
| `#errores_de_entorno`     | Problemas con librerÃ­as, rutas o entornos virtuales    | Conflictos entre kernels o rutas relativas          | Registrar en README y limpiar entornos obsoletos       |

---

# RecapitaciÃ³n de los intentos de scraping: 
**Fecha:** 26 de Agosto, 2025

## Primer intento: ["mi_proyecto_escrapeo"](https://github.com/melinnicri/Proyecto-Nombres-Empresas/tree/main/mi_proyecto_escrapeo) 
Scraping sincrÃ³nico con validaciÃ³n manual  
MÃ³dulos: `normalizaciÃ³n.py`, `correcciÃ³n.py`, `validacion.py`, `main.py`, `escrapeo.py`

---
### Objetivo  
Normalizar, corregir y validar nombres de empresas adjudicatarias para asegurar:
- Consistencia semÃ¡ntica  
- Trazabilidad tÃ©cnica  
- Transferencia reproducible  

### MÃ³dulos Implementados

1. `normalizar_nombre(nombre)`  
   - Elimina parÃ©ntesis, acentos y sÃ­mbolos  
   - Aplica sustituciones estÃ¡ndar (SAU, SLU, etc.)  
   - Convierte a mayÃºsculas y limpia espacios  

2. `corregir_nombre_con_score(nombre, diccionario)`  
   - Usa RapidFuzz (`WRatio`, `token_set_ratio`)  
   - EvalÃºa intersecciÃ³n de tokens y diferencia de longitud  
   - Devuelve mejor match y score  

3. `correccion_por_partes(nombre, diccionario)`  
   - Corrige token por token si el score global es bajo  
   - Mejora precisiÃ³n en nombres compuestos o ambiguos  

4. `pipeline_correccion(df, columna_original)`  
   - Aplica normalizaciÃ³n, correcciÃ³n y validaciÃ³n  
   - Genera columnas: `MATCH_SCORE`, `STATUS_CORRECCIÃ“N`  

5. `validacion.py`  
   - Detecta casos dudosos (`MATCH_SCORE` entre 60 y 85)  
   - Exporta correcciones sospechosas por heurÃ­stica  
   - Aplica correcciones manuales desde `revision_manual.csv`  
   - Genera log final con tipo de correcciÃ³n  

### Resultados de la IteraciÃ³n

| Empresa | CIF | URL | DirecciÃ³n | TelÃ©fono | Email |
|--------|-----|-----|-----------|----------|-------|
| ACCIONA | A95113361 | https://www.acciona.com/es | Avenida de la Gran VÃ­a de Hortaleza | 2025-08-08 | accionacorp@acciona.com |
| ACEINSA MOVILIDAD SA | A84408954 | https://aceinsa.es/web2/ | PolÃ­gono Industrial Ventorro del Cano, AlcorcÃ³n | 91 495 95 90 | aceinsa@aceinsa.es |
| AGRUPACION EUROPEA DE INDUSTRIAS DE TRANSFORMACION SL | B83037606 | https://www.iberinform.es/... | No encontrada | 83037606 | atencionclientes@iberinform.es |

### Informe de Cobertura

- Empresas procesadas: **100**  
- Con datos completos: **19**  
- Con datos parciales: **81**  
- Sin datos encontrados: **0**  
- Costo estimado por empresa Ãºtil: **1.89 horas**

### Casos Ambiguos Detectados

- Correcciones sospechosas por longitud excesiva  
- Tokens originales ausentes en el match  
- Nombres genÃ©ricos como â€œSERVICIOSâ€ o â€œGESTIÃ“Nâ€

### Aprendizajes

- La correcciÃ³n por partes mejora precisiÃ³n en nombres compuestos  
- El log por tipo de correcciÃ³n permite auditorÃ­a Ã©tica  
- La revisiÃ³n manual es clave para casos con `MATCH_SCORE` intermedio

### PrÃ³ximos Pasos

- ValidaciÃ³n geogrÃ¡fica por sede o localidad  
- Logs por empresa con evidencia de cada correcciÃ³n  
- ModularizaciÃ³n del diccionario por sector o regiÃ³n  

---
## Segundo intento: ["crawler"](https://github.com/melinnicri/Proyecto-Nombres-Empresas/tree/main/crawler)
Scraping asincrÃ³nico con CrawlerHub y fallback; 
MÃ³dulos: `normalizacion_crawler.py`, `correccion_crawler.py`, `validacion_crawler.py`, `main.py`


### DiagnÃ³stico TÃ©cnico

| Empresa | CIF | URL | DirecciÃ³n | TelÃ©fono | Email |
|---------|-----|-----|-----------|----------|-------|
| BOSTON SCIENTIFIC IBERICA SA | A80401821 | https://www.bostonscientific.com/es-ES/home.html | Error | Error | Error |
| BRENNTAG QUIMICA SAU | a59181537 | https://www.brenntag.com/es-es/ | Error | Error | Error |
| CAIXABANK SA | A08663619 | https://www.caixabank.es/particular/home/particulares_es.html | Error | Error | Error |


#### Lo que funcionÃ³:
- BÃºsqueda de URLs oficiales efectiva (6 min)  
- Scraping asincrÃ³nico activado correctamente  
- Logging por campo y por URL generado

#### Lo que no funcionÃ³:
- ExtracciÃ³n de direcciÃ³n, telÃ©fono y email fallida  
- Patrones semÃ¡nticos no encontraron contenido Ãºtil  
- HTML extraÃ­do no contenÃ­a datos estructurados

### ValidaciÃ³n por Campo â€“ IteraciÃ³n 2025-08-26

| CategorÃ­a | Empresas | DescripciÃ³n |
|-----------|----------|-------------|
| URL Ãºtil (http) | 95 | URL accesible y estructurada |
| DirecciÃ³n validada (âœ“) | 19 | Confirmada por heurÃ­stica semÃ¡ntica |
| TelÃ©fono validado (âœ“) | 53 | NÃºmero extraÃ­do y validado |
| Email validado (âœ“) | 44 | Correo electrÃ³nico extraÃ­do |
| Todos los campos validados (âœ“) | 12 | DirecciÃ³n, telÃ©fono y email confirmados |
| NingÃºn campo validado (âœ—) | 39 | Sin datos Ãºtiles |
| Solo direcciÃ³n (âœ“) | 2 | Sin telÃ©fono ni email |
| DirecciÃ³n no validada (âœ—), con contacto (âœ“) | 26 | TelÃ©fono y email sin ubicaciÃ³n |
| Solo telÃ©fono (âœ“) | 14 | Sin direcciÃ³n ni email |
| Sin telÃ©fono (âœ—), con direcciÃ³n y email (âœ“) | 4 | Sin nÃºmero |
| Solo email (âœ“) | 2 | Sin direcciÃ³n ni telÃ©fono |
| Sin email (âœ—), con direcciÃ³n y telÃ©fono (âœ“) | 1 | Sin correo |

---

## Tercer Intento: ["scraper_final"](https://github.com/melinnicri/Proyecto-Nombres-Empresas/tree/main/version_02)
Scraping asincrÃ³nico con Playwright refinado, validaciÃ³n semÃ¡ntica por campo y logging modular  
MÃ³dulos: VersiÃ³n_02/nombres_scraping: buscar_url.py, urls.csv; mÃ³dulos_contacto: extractor.py, pipeline_contacto.py, contacto.csv; anÃ¡lisis: estadÃ­stica.ipynb.

### Objetivo  
Extraer datos de contacto empresariales con mÃ¡xima cobertura por campo, validaciÃ³n semÃ¡ntica y trazabilidad reproducible.  
Se prioriza la auditorÃ­a por campo, el logging por empresa y la revisiÃ³n manual de casos ambiguos (normalizaciÃ³n y correciÃ³n de los nombres se compartiÃ³ desde el segundo intento).

### Mejoras Implementadas

- Scraping asincrÃ³nico con control de concurrencia (`asyncio.Semaphore`)  
- ValidaciÃ³n semÃ¡ntica por campo (`DirecciÃ³n`, `TelÃ©fono`, `Email`)  
- Logging modular por empresa y por tipo de error  
- RevisiÃ³n manual integrada en el pipeline (`revision_manual.csv`)  
- ExportaciÃ³n reproducible con trazabilidad por campo

### Resultados de la IteraciÃ³n
Resultados en contacto.csv:

| empresa | url | email | telefono | direccion | error |
|---------|-----|-------|----------|-----------|-------| 
| PROYECTOS DE INGENIERIA EXTREMENOS SL | https://www.prodiex.com/ | info@prodiex.com | 924 303 647 | No encontrado |
| UNION PROTECCION CIVIL SL | http://unionproteccioncivil.es/contact | administracion@unionproteccioncivil.es | 967 66 36 | 
"Avenida Isabel la CatÃ³lica 1c-d 02005 Albacete; Calle Velazquez, 8628001 Madrid; Carrer Gremi Fusters, 3307009 Palma; Calle Trinidad Grund, 2129001 MÃ¡laga" |
| BEY BAIZAN FRANCISCO JAVIER | https://www.boe.es/gazeta/dias/1905/07/14/pdfs/GMD-1905-195.pdf |||| 
"Page.goto: net::ERR_ABORTED at https://www.boe.es/gazeta/dias/1905/07/14/pdfs/GMD-1905-195.pdf
Call log:
  - navigating to ""https://www.boe.es/gazeta/dias/1905/07/14/pdfs/GMD-1905-195.pdf"" | waiting until ""load""
" |

### Informe de Cobertura
Datos de contacto extraÃ­dos:

| Campo | Coincidencias | Porcentaje |
|-------|---------------|------------|
| DirecciÃ³n | 6/100 | 6.00% |
| TelÃ©fono	| 69/100 | 69.00% |
| Email	| 54/100	| 54.00% |

- Tiempo promedio por empresa Ãºtil: **1.12 horas**

Se realizÃ³:
1. Pipeline asincrÃ³nico para bÃºsqueda de URLs oficiales
â€¢	Usa googlesearch con validaciÃ³n semÃ¡ntica opcional (modo_estricto).
â€¢	Filtra dominios irrelevantes y valida contenido si se requiere.
â€¢	Deja trazabilidad por empresa, estado y URL encontrada.
Impacto: evita falsos positivos y permite auditar cada resultado por tipo de validaciÃ³n.
2. Extractor de contacto con BeautifulSoup y regex robusto
â€¢	Busca en secciones semÃ¡nticas (footer, div, p, span) con clases tipo contact, info, footer.
â€¢	Aplica regex para email, telÃ©fono y direcciÃ³n con validaciÃ³n por localidad.
Impacto: permite extraer datos incluso en HTML desordenado, y filtra direcciones por contexto geogrÃ¡fico.
3. Scraper asincrÃ³nico con Playwright y semÃ¡foro de concurrencia
â€¢	Navega a posibles pÃ¡ginas de contacto (/contacto, /about-us, etc.).
â€¢	Extrae contenido HTML y lo pasa al extractor.
â€¢	Maneja errores como TimeoutError, TargetClosedError, y deja trazabilidad por empresa.
Impacto: evita saturaciÃ³n de recursos, permite scraping Ã©tico y deja evidencia por cada intento.

# ComparaciÃ³n tÃ©cnica entre los tres intentos de scraping:

| MÃ©trica / Criterio	Intento 1: SincrÃ³nico + validaciÃ³n manual	| Intento 2: AsincrÃ³nico + CrawlerHub + fallback	| Intento 3: AsincrÃ³nico modular + Playwright |
| Empresas procesadas	| 100	| 100	| 100 |
|URLs Ãºtiles encontradas (http)	| 78	| 95	| 100 âœ… |
| Emails validados	| 36	| 44	| 54 âœ… |
| TelÃ©fonos validados	| 41	| 53	| 69 âœ… |
| Direcciones validadas	| 12	| 19	| 6 |
| Empresas con todos los campos validados	| 19	| 12	| â€¦ |
| Empresas sin ningÃºn dato Ãºtil	| 0	| 39	| 0 |
| Logging por campo	| No	| Parcial	| SÃ­ (por campo + por empresa) âœ… |
| ExportaciÃ³n reproducible (contacto.csv)	| No	| Parcial	| SÃ­, con trazabilidad âœ… |
| CorrecciÃ³n fonÃ©tica y por partes	| Parcial	| Mejorada	| Completa + overrides manuales âœ… |
| Manejo de errores documentado	| Parcial	| Parcial	| SÃ­ (ej. ERR_ABORTED) âœ… |
| Tiempo de ejecuciÃ³n	| Alto (manual)	| Bajo (6 min)	| Medio (3.5 min) |

---
# ConclusiÃ³n basada en evidencia
El tercer intento es el mÃ¡s completo y tÃ©cnicamente sÃ³lido. Supera a los anteriores en:
â€¢	Cobertura de datos Ãºtiles
â€¢	Modularidad del pipeline
â€¢	ExportaciÃ³n reproducible
â€¢	Logging y trazabilidad
â€¢	CorrecciÃ³n fonÃ©tica y validaciÃ³n semÃ¡ntica

Finalmente, la tabla anterior resume los tres intentos de scraping realizados, comparando mÃ©tricas tÃ©cnicas verificables. 
El tercer intento, basado en un pipeline asincrÃ³nico modular con Playwright, presenta la mayor cobertura por campo, 
mejor trazabilidad y exportaciÃ³n reproducible. Aunque algunos valores no fueron calculados con precisiÃ³n absoluta, 
la evidencia disponible permite concluir que esta versiÃ³n es la mÃ¡s robusta y reproducible del proceso.

---

## Extras:

---

```

# Diagrama del proyecto:
Proyecto-Nombres-Empresas/mi_proyecto_escrapeo
â”œâ”€â”€ image/
â”‚   â””â”€â”€ scrapeo/
â”œâ”€â”€ mi_proyecto_escrapeo/
â”‚   â”œâ”€â”€ escrapeo/
â”‚   â”‚   â”œâ”€â”€ data/                                                    # Archivos CSV de entrada y salida
â”‚   â”‚   â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ contacto_empresas_es.csv
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ empresas_completas.csv
â”‚   â”‚   â”‚   â””â”€â”€ raw/
â”‚   â”‚   â”‚       â””â”€â”€ comparados.csv
â”‚   â”‚   â”œâ”€â”€ src/                                                      # Scripts del flujo tÃ©cnico
â”‚   â”‚   â”‚   â””â”€â”€ comparativa.ipynb
â”‚   â”‚   â”œâ”€â”€ escrapeo_1.py
â”‚   â”‚   â””â”€â”€ requirements.txt                                          # LibrerÃ­as necesarias para reproducir el entorno para escrapeo
â”‚   â””â”€â”€ proyecto_correccion/
â”‚       â”œâ”€â”€ __pycache__/
â”‚       â”‚   â”œâ”€â”€ correccion.cpython-312.pyc
â”‚       â”‚   â”œâ”€â”€ normalizacion.cpython-312.pyc
â”‚       â”‚   â””â”€â”€ validacion.cpython-312.pyc
â”‚       â”œâ”€â”€ data/                                                      # Archivos CSV de entrada y salida
â”‚       â”‚   â”œâ”€â”€ processed/
â”‚       â”‚   â”‚   â”œâ”€â”€ empresas_limpias_corregidas_final.csv
â”‚       â”‚   â”‚   â””â”€â”€ empresas_limpias_corregidas_mejorado.csv
â”‚       â”‚   â””â”€â”€ raw/
â”‚       â”‚       â””â”€â”€ 100empresas.csv
â”‚       â”œâ”€â”€ logs/                                                       # Logs funcionales de la correciÃ³n de los nombres de empresas
â”‚       â”‚   â”œâ”€â”€ correcciones_sospechosas.csv
â”‚       â”‚   â”œâ”€â”€ log_de_correcciones.csv
â”‚       â”‚   â””â”€â”€ revision_manual.csv
â”‚       â”œâ”€â”€ src/                                                        # Scripts del flujo tÃ©cnico
â”‚       â”‚   â”œâ”€â”€ modules/
â”‚       â”‚   â”‚   â”œâ”€â”€ correccion.py
â”‚       â”‚   â”‚   â”œâ”€â”€ normalizacion.py
â”‚       â”‚   â”‚   â””â”€â”€ validacion.py
â”‚       â”‚   â””â”€â”€ main.py
â”‚       â””â”€â”€ requirements.txt                                            # LibrerÃ­as necesarias para reproducir el entorno para correcciÃ³n
â”œâ”€â”€ README.md                                                           # Informe tÃ©cnico en formato Markdown


```

---

```
## Diagrama de flujo funcional del proyecto

Inicio
  â†“
ğŸ“ Carga de datos iniciales (100empresas.csv)
  â†“
ğŸ§¹ CorrecciÃ³n fonÃ©tica y semÃ¡ntica
  â”œâ”€> correccion.py
  â”œâ”€> normalizacion.py
  â””â”€> validacion.py
  â†“
ğŸ“Š GeneraciÃ³n de empresas_limpias_corregidas_final.csv
  â†“
ğŸ“ Registro de correcciones en logs/*.csv
  â†“
ğŸŒ Scrapeo de nombres corregidos
  â”œâ”€> escrapeo_1.py
  â””â”€> comparativa.ipynb
  â†“
ğŸ“ Almacenamiento en contacto_empresas_es.csv y empresas_completas.csv
  â†“
ğŸ“š DocumentaciÃ³n en README.md
  â†“
Fin

```
---



```
## Diagrama de la carpeta "crawler" (escrapeo hÃ­brido, asincrÃ³nico, con fallback dinÃ¡mico y validadciÃ³n semÃ¡ntica, realizado con Crawl4ai).

crawler/
â”œâ”€â”€ __pycache__/                     # Archivos compilados automÃ¡ticamente por Python
â”‚   â”œâ”€â”€ contact_extractor.cpython-313.pyc
â”‚   â”œâ”€â”€ correccion_crawler.cpython-313.pyc
â”‚   â”œâ”€â”€ diccionario_manual.cpython-313.pyc
â”‚   â”œâ”€â”€ gestor_correcciones.cpython-313.pyc
â”‚   â”œâ”€â”€ normalizacion_crawler.cpython-313.pyc
â”‚   â”œâ”€â”€ scraping_utils.cpython-313.pyc
â”‚   â””â”€â”€ validacion_crawler.cpython-313.pyc

â”œâ”€â”€ data/                            # Ecosistema de datos
â”‚   â”œâ”€â”€ raw/                         # Datos originales sin procesar
â”‚   â”‚   â””â”€â”€ 100empresas.csv
â”‚   â”œâ”€â”€ processed/                   # Datos corregidos, validados y listos para anÃ¡lisis
â”‚   â”‚   â”œâ”€â”€ contacto_empresas_es_2.csv*
â”‚   â”‚   â”œâ”€â”€ correcciones_sospechosas.csv
â”‚   â”‚   â”œâ”€â”€ diccionario_nombres_corregidos.csv
â”‚   â”‚   â”œâ”€â”€ empresas_limpias_corregidas_final.csv
â”‚   â”‚   â”œâ”€â”€ empresas_limpias_corregidas_mejorado.csv
â”‚   â”‚   â”œâ”€â”€ nombres_normalizados_para_scraping.csv
â”‚   â”‚   â””â”€â”€ revision_manual.csv
â”‚   â””â”€â”€ diccionario_manual.py        # Correcciones manuales y mapeos heurÃ­sticos

â”œâ”€â”€ logs/                            # AuditorÃ­a del proceso y scripts auxiliares
â”‚   â”œâ”€â”€ auditoria_texto_extraido.py
â”‚   â”œâ”€â”€ contact_extractor.py
â”‚   â”œâ”€â”€ gestor_correcciones.py
â”‚   â”œâ”€â”€ log_de_correcciones.csv
â”‚   â”œâ”€â”€ main_async.py
â”‚   â””â”€â”€ scraping_utils.py

â”œâ”€â”€ src/                             # CÃ³digo principal y mÃ³dulos reutilizables
â”‚   â”œâ”€â”€ modules/                     # Funciones compartidas y componentes tÃ©cnicos
â”‚   â””â”€â”€ main.py                      # Script principal de ejecuciÃ³n

â”œâ”€â”€ compar2.ipynb                    # Notebook de comparaciÃ³n o anÃ¡lisis exploratorio
â”œâ”€â”€ escrapeo_dos.py                  # Script de scraping hÃ­brido con fallback dinÃ¡mico
â””â”€â”€ requirements.txt                 # Lista de dependencias del proyecto

Resultado: extrae los urls con pocos errores, pero direcciÃ³n, telÃ©fono y email no los alcanza a extraer. En 36 segundos.
```


```
AgreguÃ© otra versiÃ³n mejorada del sistema asÃ­ncrona de scraping de informaciÃ³n de empresas espaÃ±olas: version_02
VersiÃ³n_02/
â”œâ”€â”€ nombres_scraping/                  # Punto de partida del proceso
â”‚   â””â”€â”€ buscar_url.py                  # Sistema asincrÃ³nico que busca URLs
â”‚   â””â”€â”€ urls.csv                       # Resultado exitoso (Â¡por fin!)
â”‚
â”œâ”€â”€ mÃ³dulos_contacto/                 # ExtracciÃ³n y validaciÃ³n de contacto
â”‚   â””â”€â”€ extractor.py                  # Auxiliar para extracciÃ³n
â”‚   â””â”€â”€ pipeline_contacto.py          # Pipeline principal
â”‚   â””â”€â”€ contacto.csv                  # Resultado con datos de contacto
â”‚
â”œâ”€â”€ anÃ¡lisis/                         # EstadÃ­sticas y validaciones
â”‚   â””â”€â”€ estadÃ­stica.ipynb             # Notebook con anÃ¡lisis de urls y contacto
â”œâ”€â”€ requirements.txt                  # Requerimientos del entorno virtual para trabajarlo
```
