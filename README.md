# Prueba T√©cnica: Scraping de Informaci√≥n en P√°gina Web

**Extracci√≥n automatizada de datos de contacto empresarial desde fuentes p√∫blicas**  
**Autora:** Amelia Cristina Herrera Brice√±o  
**Rol:** Data Analyst & Scientist en transici√≥n hacia BI y automatizaci√≥n  
**Entorno de trabajo:** Python ¬∑ Pandas ¬∑ BeautifulSoup ¬∑ Regex ¬∑ CSV ¬∑ Exploraci√≥n reproducible  
**Fecha de entrega:** 15 Agosto 2025  

<p align="center"><img src="https://github.com/melinnicri/Proyecto-Nombres-Empresas/blob/main/image/scrapeo.png"></p>

---

## 1. Problema

Se cuenta con listados de empresas (nombre y CIF), pero se carece de sus datos de contacto directo 
(emails corporativos, tel√©fonos, direcciones). Para establecer comunicaci√≥n comercial, es necesario 
localizar y extraer autom√°ticamente esta informaci√≥n desde sus sitios web oficiales u otras fuentes 
p√∫blicas disponibles en internet.

---

## 2. Soluci√≥n T√©cnica

Desarrollar un proceso de scraping web que permita obtener datos de contacto empresariales de forma 
automatizada a partir de los nombres de las empresas.

---

## 3. Alcance de la Prueba

- **Dataset:** Muestra de 50‚Äì100 empresas espa√±olas (nombre + CIF)
- **Proceso esperado:**
  1. Localizar la web corporativa oficial de cada empresa
  2. Extraer datos de contacto mediante scraping web
- **Datos objetivo:**
  - URL oficial
  - Email corporativo principal (info@, contacto@, comercial@)
  - Tel√©fono de contacto
  - Direcci√≥n postal completa
- **Entregables:**
  - Archivo CSV/Excel con los datos obtenidos
  - Documentaci√≥n t√©cnica del proceso
  - Registro de limitaciones (robots.txt, captchas, etc.)
  - Tiempo estimado de procesamiento por empresa
  - Coste estimado del proceso
  - Video explicativo del flujo t√©cnico

---

## 4. Implementaci√≥n T√©cnica

- Scraping sincr√≥nico reproducible: `requests` + `BeautifulSoup` + `regex`
- Limpieza post-scraping: DataFrames exportables, validaci√≥n de encoding y estructura
- Exportaci√≥n: Archivos CSV listos para an√°lisis y visualizaci√≥n

---

## 5. M√©tricas y Evaluaci√≥n

- **Tasa de √©xito:** porcentaje de empresas con datos completos
- **Limitaciones encontradas:** robots.txt, captchas, p√°ginas sin datos visibles
- **Tiempo estimado por empresa:** medido con barra de progreso
- **Coste estimado:** bajo en entorno local; escalable con APIs

---

## 6. Video Explicativo

üé• Se graba un video mostrando el flujo, explicando el c√≥digo y los resultados obtenidos.  
_Pendiente de edici√≥n y subida._

---

## 7. Escalabilidad

Para flujos masivos, se propone una versi√≥n asincr√≥nica con `aiohttp` o `crawl4ai`, que mejora el 
rendimiento y permite scraping concurrente.

---

## 8. Comparativa de M√©todos de B√∫squeda de URLs Oficiales


| M√©todo                    | Gratuito | L√≠mite         | Costo estimado | Ventajas                            | Desventajas                           |
|---------------------------|----------|----------------|----------------|-------------------------------------|---------------------------------------|
| googlesearch (scraping)   | S√≠       | No oficial     | $0             | Sin registro, ideal para pruebas    | Bloqueos, resultados no estructurados |
| SerpAPI                   | S√≠ (250) | Pago desde $50 | ~$50           | JSON limpio, sin bloqueo            | Requiere API Key, coste mensual       |
| Zenserp                   | S√≠ (50)  | Pago desde $29 | ~$29           | F√°cil de usar, resultados precisos  | Menor volumen gratuito                |
| Google Custom Search API  | S√≠ (100) | $5/1000        | ~$5            | Oficial, configurable               | Requiere configuraci√≥n previa         |
| Bing Search API           | S√≠ (1000)| Pago desde $3  | ~$3            | Econ√≥mica, buena cobertura          | Menor precisi√≥n en empresas locales   |

---

## 9. Recomendaci√≥n Final

Para esta prueba t√©cnica (50‚Äì100 empresas), se recomienda utilizar `googlesearch` por su simplicidad 
y coste cero. Para escalar el proceso, se sugiere evaluar APIs comerciales como `SerpAPI` y adoptar 
scraping asincr√≥nico con `aiohttp` o `crawl4ai`.

---

## 10. Correcci√≥n Fon√©tica y Sem√°ntica de Nombres Empresariales

Objetivo: corregir nombres empresariales con validaci√≥n fon√©tica (`rapidfuzz`) y sem√°ntica (`unidecode`), 
asegurando trazabilidad y limpieza formal.

### Ejemplo de correcci√≥n fon√©tica

```python
import re
from unidecode import unidecode

def normalizar_nombre(nombre):
    nombre = unidecode(nombre)
    nombre = re.sub(r'[^\w\s]', '', nombre)
    nombre = nombre.strip().upper()
    return nombre

nombre_1 = "Compa√±√≠a ABC S.A."
nombre_2 = "Compania - ABC, SA"

print(normalizar_nombre(nombre_1))  # COMPANIA ABC SA
print(normalizar_nombre(nombre_2))  # COMPANIA ABC SA
```

---

## 11. Flujo de Correcci√≥n y Validaci√≥n

Este m√≥dulo se encarga de limpiar, corregir y validar los nombres empresariales antes del scraping,
asegurando trazabilidad y coherencia sem√°ntica.

| Archivo           | Prop√≥sito principal                                                         |
|-------------------|-----------------------------------------------------------------------------|
| `normalizacion.py`| Funciones para limpiar y normalizar nombres (acentos, s√≠mbolos, may√∫sculas) |
| `correccion.py`   | Correcci√≥n fon√©tica con `RapidFuzz` y validaci√≥n sem√°ntica con `unidecode`  |
| `validacion.py`   | Detecci√≥n de correcciones sospechosas y generaci√≥n de revisi√≥n manual       |
| `main.py`         | Orquestaci√≥n del flujo completo: carga, limpieza, correcci√≥n y exportaci√≥n  |

> Cada paso del flujo est√° documentado y modularizado, permitiendo auditor√≠a, mejora continua y ense√±anza t√©cnica.

---

## 12. Archivos CSV y Logs Funcionales

| Archivo CSV                                | Rol en el flujo                            | Contenido esperado                             | Sugerencias de mejora                              |
|--------------------------------------------|--------------------------------------------|------------------------------------------------|----------------------------------------------------|
| `empresas_limpias_corregidas_mejorado.csv` | Post-normalizaci√≥n y correcci√≥n autom√°tica | Nombres limpios + correcciones autom√°ticas     | Agregar columna `ORIGEN_CORRECCI√ìN` y `FECHA_PROCESO`       |
| `revision_manual.csv`                      | Correcciones manuales aplicadas            | Casos l√≠mite revisados por uno mismo           | Agregar columna `OBSERVACIONES` y `VALIDACI√ìN_MANUAL`   |
| `correcciones_sospechosas.csv`             | Casos con ambig√ºedad o errores detectados  | Correcciones dudosas, posibles sobreajustes    | Agregar columna `TIPO_ERROR` y `RECOMENDACI√ìN`                            |
| `empresas_limpias_corregidas_final.csv`    | Resultado final validado                   | Nombres corregidos y validados                 | Agregar columna `VALIDACI√ìN_FINAL` y `FUENTE_CORRECCI√ìN`   |
| `log_de_correcciones.csv`                  | Registro trazable de cada correcci√≥n       | Entradas, salidas, tipo de correcci√≥n          | Ya bien estructurado, solo falta `ID` y `TIMESTAMP`                       |

---

## 13. Informe de Cobertura y Limitaciones del Scraping

- **Empresas procesadas:** 100  
- **Con datos completos:** 19  
- **Con datos parciales:** 81  
- **Sin datos encontrados:** 0  
- **Costo estimado por empresa √∫til:** 1.89 horas  

---

### Hallazgos clave

1. Datos incompletos en la p√°gina principal  
2. Carga asincr√≥nica de contenido  
3. URLs inv√°lidas o redireccionadas  
4. Datos mezclados o mal formateados  

---

## 14. Mejoras Sugeridas

- Ajustar patrones de `regex`  
- Filtrar URLs sin `http`  
- Documentar casos sin datos  
- Explorar subp√°ginas autom√°ticamente  
- Usar `Selenium` o `Playwright`  
- Consultar APIs internas  
- Registrar logs por empresa  

---

## 15. Reflexi√≥n Final

Este ejercicio permiti√≥ identificar las limitaciones del scraping tradicional y la necesidad de
modularizar el flujo. Se documentaron errores como evidencia de aprendizaje y se dejaron mejoras
futuras para auditor√≠a y ense√±anza.

---

## 16. Tiempo y Esfuerzo Invertido

**Tareas realizadas:**

- Preparaci√≥n del entorno virtual  
- Limpieza y validaci√≥n de nombres  
- Gesti√≥n de asincron√≠a y subp√°ginas  
- Documentaci√≥n de incidentes  
- Comparaci√≥n de flujos  
- Generaci√≥n de evidencia  

**Tiempo total invertido:** ~36 horas distribuidas en 3 d√≠as intensivos

---

## 17. Tabla de Incidentes T√©cnicos y Validaciones

| Tipo de incidente         | Descripci√≥n breve                                      | Ejemplo o patr√≥n detectado                          | Mejora futura sugerida                                 |
|---------------------------|--------------------------------------------------------|-----------------------------------------------------|--------------------------------------------------------|
| `#asincron√≠a`             | Carga de datos depende de JavaScript o peticiones din√°micas | P√°ginas sin datos en HTML est√°tico              | Usar `Selenium` o capturas manuales                    |
| `#estructura_inaccesible` | Datos embebidos en im√°genes o banners                  | Contacto solo en PDF o banners                      | Documentar como ‚Äúno extra√≠ble autom√°ticamente‚Äù         |
| `#subp√°gina_oculta`       | Datos en subp√°ginas no enlazadas desde el home         | Secci√≥n ‚ÄúContacto‚Äù sin enlace directo               | Detectar enlaces internos relevantes                   |
| `#validaci√≥n_manual`      | Requiere criterio humano para interpretar ambig√ºedades | Nombres gen√©ricos o duplicados                      | Crear diccionario personalizado y registrar ejemplos   |
| `#datos_incompletos`      | Se extrae solo parte del contacto                      | Formularios sin email visible                       | Marcar como ‚Äúparcial‚Äù y registrar en log               |
| `#estructura_inconsistente`| HTML var√≠a entre empresas, dificultando selectores √∫nicos | Plantillas distintas o CMS personalizado         | Modularizar el scraping por tipo de estructura         |
| `#bloqueo_scraping`       | El sitio detecta scraping y bloquea o redirige         | Cloudflare, captcha o redirecci√≥n forzada           | Documentar como ‚Äúrequiere intervenci√≥n manual‚Äù         |
| `#errores_de_entorno`     | Problemas con librer√≠as, rutas o entornos virtuales    | Conflictos entre kernels o rutas relativas          | Registrar en README y limpiar entornos obsoletos       |

---

# Recapitaci√≥n de los intentos de scraping: 
**Fecha:** 26 de Agosto, 2025

## Primer intento: ["mi_proyecto_escrapeo"](https://github.com/melinnicri/Proyecto-Nombres-Empresas/tree/main/mi_proyecto_escrapeo) 
Scraping sincr√≥nico con validaci√≥n manual  
M√≥dulos: `normalizaci√≥n.py`, `correcci√≥n.py`, `validacion.py`, `main.py`, `escrapeo.py`

---
### Objetivo  
Normalizar, corregir y validar nombres de empresas adjudicatarias para asegurar:
- Consistencia sem√°ntica  
- Trazabilidad t√©cnica  
- Transferencia reproducible  

### M√≥dulos Implementados

1. `normalizar_nombre(nombre)`  
   - Elimina par√©ntesis, acentos y s√≠mbolos  
   - Aplica sustituciones est√°ndar (SAU, SLU, etc.)  
   - Convierte a may√∫sculas y limpia espacios  

2. `corregir_nombre_con_score(nombre, diccionario)`  
   - Usa RapidFuzz (`WRatio`, `token_set_ratio`)  
   - Eval√∫a intersecci√≥n de tokens y diferencia de longitud  
   - Devuelve mejor match y score  

3. `correccion_por_partes(nombre, diccionario)`  
   - Corrige token por token si el score global es bajo  
   - Mejora precisi√≥n en nombres compuestos o ambiguos  

4. `pipeline_correccion(df, columna_original)`  
   - Aplica normalizaci√≥n, correcci√≥n y validaci√≥n  
   - Genera columnas: `MATCH_SCORE`, `STATUS_CORRECCI√ìN`  

5. `validacion.py`  
   - Detecta casos dudosos (`MATCH_SCORE` entre 60 y 85)  
   - Exporta correcciones sospechosas por heur√≠stica  
   - Aplica correcciones manuales desde `revision_manual.csv`  
   - Genera log final con tipo de correcci√≥n  

### Resultados de la Iteraci√≥n

| Empresa | CIF | URL | Direcci√≥n | Tel√©fono | Email |
|--------|-----|-----|-----------|----------|-------|
| ACCIONA | A95113361 | https://www.acciona.com/es | Avenida de la Gran V√≠a de Hortaleza | 2025-08-08 | accionacorp@acciona.com |
| ACEINSA MOVILIDAD SA | A84408954 | https://aceinsa.es/web2/ | Pol√≠gono Industrial Ventorro del Cano, Alcorc√≥n | 91 495 95 90 | aceinsa@aceinsa.es |
| AGRUPACION EUROPEA DE INDUSTRIAS DE TRANSFORMACION SL | B83037606 | https://www.iberinform.es/... | No encontrada | 83037606 | atencionclientes@iberinform.es |

### Informe de Cobertura

- Empresas procesadas: **100**  
- Con datos completos: **19**  
- Con datos parciales: **81**  
- Sin datos encontrados: **0**  
- Costo estimado por empresa √∫til: **1.89 horas**

### Casos Ambiguos Detectados

- Correcciones sospechosas por longitud excesiva  
- Tokens originales ausentes en el match  
- Nombres gen√©ricos como ‚ÄúSERVICIOS‚Äù o ‚ÄúGESTI√ìN‚Äù

### Aprendizajes

- La correcci√≥n por partes mejora precisi√≥n en nombres compuestos  
- El log por tipo de correcci√≥n permite auditor√≠a √©tica  
- La revisi√≥n manual es clave para casos con `MATCH_SCORE` intermedio

### Pr√≥ximos Pasos

- Validaci√≥n geogr√°fica por sede o localidad  
- Logs por empresa con evidencia de cada correcci√≥n  
- Modularizaci√≥n del diccionario por sector o regi√≥n  

---
## Segundo intento: ["crawler"](https://github.com/melinnicri/Proyecto-Nombres-Empresas/tree/main/crawler)
Scraping asincr√≥nico con CrawlerHub y fallback; 
M√≥dulos: `normalizacion_crawler.py`, `correccion_crawler.py`, `validacion_crawler.py`, `main.py`


### Diagn√≥stico T√©cnico

| Empresa | CIF | URL | Direcci√≥n | Tel√©fono | Email |
|---------|-----|-----|-----------|----------|-------|
| BOSTON SCIENTIFIC IBERICA SA | A80401821 | https://www.bostonscientific.com/es-ES/home.html | Error | Error | Error |
| BRENNTAG QUIMICA SAU | a59181537 | https://www.brenntag.com/es-es/ | Error | Error | Error |
| CAIXABANK SA | A08663619 | https://www.caixabank.es/particular/home/particulares_es.html | Error | Error | Error |


#### Lo que funcion√≥:
- B√∫squeda de URLs oficiales efectiva (6 min)  
- Scraping asincr√≥nico activado correctamente  
- Logging por campo y por URL generado

#### Lo que no funcion√≥:
- Extracci√≥n de direcci√≥n, tel√©fono y email fallida  
- Patrones sem√°nticos no encontraron contenido √∫til  
- HTML extra√≠do no conten√≠a datos estructurados

### Validaci√≥n por Campo ‚Äì Iteraci√≥n 2025-08-26

| Categor√≠a | Empresas | Descripci√≥n |
|-----------|----------|-------------|
| URL √∫til (http) | 95 | URL accesible y estructurada |
| Direcci√≥n validada (‚úì) | 19 | Confirmada por heur√≠stica sem√°ntica |
| Tel√©fono validado (‚úì) | 53 | N√∫mero extra√≠do y validado |
| Email validado (‚úì) | 44 | Correo electr√≥nico extra√≠do |
| Todos los campos validados (‚úì) | 12 | Direcci√≥n, tel√©fono y email confirmados |
| Ning√∫n campo validado (‚úó) | 39 | Sin datos √∫tiles |
| Solo direcci√≥n (‚úì) | 2 | Sin tel√©fono ni email |
| Direcci√≥n no validada (‚úó), con contacto (‚úì) | 26 | Tel√©fono y email sin ubicaci√≥n |
| Solo tel√©fono (‚úì) | 14 | Sin direcci√≥n ni email |
| Sin tel√©fono (‚úó), con direcci√≥n y email (‚úì) | 4 | Sin n√∫mero |
| Solo email (‚úì) | 2 | Sin direcci√≥n ni tel√©fono |
| Sin email (‚úó), con direcci√≥n y tel√©fono (‚úì) | 1 | Sin correo |

---

## Tercer Intento: ["scraper_final"](https://github.com/melinnicri/Proyecto-Nombres-Empresas/tree/main/version_02)
Scraping asincr√≥nico con Playwright refinado, validaci√≥n sem√°ntica por campo y logging modular  
M√≥dulos: Versi√≥n_02/nombres_scraping: buscar_url.py, urls.csv; m√≥dulos_contacto: extractor.py, pipeline_contacto.py, contacto.csv; an√°lisis: estad√≠stica.ipynb.

### Objetivo  
Extraer datos de contacto empresariales con m√°xima cobertura por campo, validaci√≥n sem√°ntica y trazabilidad reproducible.  
Se prioriza la auditor√≠a por campo, el logging por empresa y la revisi√≥n manual de casos ambiguos (normalizaci√≥n y correci√≥n de los nombres se comparti√≥ desde el segundo intento).

### Mejoras Implementadas

- Scraping asincr√≥nico con control de concurrencia (`asyncio.Semaphore`)  
- Validaci√≥n sem√°ntica por campo (`Direcci√≥n`, `Tel√©fono`, `Email`)  
- Logging modular por empresa y por tipo de error  
- Revisi√≥n manual integrada en el pipeline (`revision_manual.csv`)  
- Exportaci√≥n reproducible con trazabilidad por campo

### Resultados de la Iteraci√≥n
Resultados en contacto.csv:

| empresa | url | email | telefono | direccion | error |
|---------|-----|-------|----------|-----------|-------| 
| PROYECTOS DE INGENIERIA EXTREMENOS SL | https://www.prodiex.com/ | info@prodiex.com | 924 303 647 | No encontrado |
| UNION PROTECCION CIVIL SL | http://unionproteccioncivil.es/contact | administracion@unionproteccioncivil.es | 967 66 36 | 
"Avenida Isabel la Cat√≥lica 1c-d 02005 Albacete; Calle Velazquez, 8628001 Madrid; Carrer Gremi Fusters, 3307009 Palma; Calle Trinidad Grund, 2129001 M√°laga" |
| BEY BAIZAN FRANCISCO JAVIER | https://www.boe.es/gazeta/dias/1905/07/14/pdfs/GMD-1905-195.pdf |||| 
"Page.goto: net::ERR_ABORTED at https://www.boe.es/gazeta/dias/1905/07/14/pdfs/GMD-1905-195.pdf
Call log:
  - navigating to ""https://www.boe.es/gazeta/dias/1905/07/14/pdfs/GMD-1905-195.pdf"" | waiting until ""load""
" |

### Informe de Cobertura
Datos de contacto extra√≠dos:

| Campo | Coincidencias | Porcentaje |
|-------|---------------|------------|
| Direcci√≥n | 6/100 | 6.00% |
| Tel√©fono	| 69/100 | 69.00% |
| Email	| 54/100	| 54.00% |

- Tiempo promedio por empresa √∫til: **1.12 horas**

Se realiz√≥:
1. Pipeline asincr√≥nico para b√∫squeda de URLs oficiales
‚Ä¢	Usa googlesearch con validaci√≥n sem√°ntica opcional (modo_estricto).
‚Ä¢	Filtra dominios irrelevantes y valida contenido si se requiere.
‚Ä¢	Deja trazabilidad por empresa, estado y URL encontrada.
Impacto: evita falsos positivos y permite auditar cada resultado por tipo de validaci√≥n.
2. Extractor de contacto con BeautifulSoup y regex robusto
‚Ä¢	Busca en secciones sem√°nticas (footer, div, p, span) con clases tipo contact, info, footer.
‚Ä¢	Aplica regex para email, tel√©fono y direcci√≥n con validaci√≥n por localidad.
Impacto: permite extraer datos incluso en HTML desordenado, y filtra direcciones por contexto geogr√°fico.
3. Scraper asincr√≥nico con Playwright y sem√°foro de concurrencia
‚Ä¢	Navega a posibles p√°ginas de contacto (/contacto, /about-us, etc.).
‚Ä¢	Extrae contenido HTML y lo pasa al extractor.
‚Ä¢	Maneja errores como TimeoutError, TargetClosedError, y deja trazabilidad por empresa.
Impacto: evita saturaci√≥n de recursos, permite scraping √©tico y deja evidencia por cada intento.

# Comparaci√≥n t√©cnica entre los tres intentos de scraping:

| M√©trica / Criterio	Intento 1: Sincr√≥nico + validaci√≥n manual	| Intento 2: Asincr√≥nico + CrawlerHub + fallback	| Intento 3: Asincr√≥nico modular + Playwright |
| Empresas procesadas	| 100	| 100	| 100 |
|URLs √∫tiles encontradas (http)	| 78	| 95	| 100 ‚úÖ |
| Emails validados	| 36	| 44	| 54 ‚úÖ |
| Tel√©fonos validados	| 41	| 53	| 69 ‚úÖ |
| Direcciones validadas	| 12	| 19	| 6 |
| Empresas con todos los campos validados	| 19	| 12	| ‚Ä¶ |
| Empresas sin ning√∫n dato √∫til	| 0	| 39	| 0 |
| Logging por campo	| No	| Parcial	| S√≠ (por campo + por empresa) ‚úÖ |
| Exportaci√≥n reproducible (contacto.csv)	| No	| Parcial	| S√≠, con trazabilidad ‚úÖ |
| Correcci√≥n fon√©tica y por partes	| Parcial	| Mejorada	| Completa + overrides manuales ‚úÖ |
| Manejo de errores documentado	| Parcial	| Parcial	| S√≠ (ej. ERR_ABORTED) ‚úÖ |
| Tiempo de ejecuci√≥n	| Alto (manual)	| Bajo (6 min)	| Medio (3.5 min) |

---
# Conclusi√≥n basada en evidencia
El tercer intento es el m√°s completo y t√©cnicamente s√≥lido. Supera a los anteriores en:
‚Ä¢	Cobertura de datos √∫tiles
‚Ä¢	Modularidad del pipeline
‚Ä¢	Exportaci√≥n reproducible
‚Ä¢	Logging y trazabilidad
‚Ä¢	Correcci√≥n fon√©tica y validaci√≥n sem√°ntica

Finalmente, la tabla anterior resume los tres intentos de scraping realizados, comparando m√©tricas t√©cnicas verificables. 
El tercer intento, basado en un pipeline asincr√≥nico modular con Playwright, presenta la mayor cobertura por campo, 
mejor trazabilidad y exportaci√≥n reproducible. Aunque algunos valores no fueron calculados con precisi√≥n absoluta, 
la evidencia disponible permite concluir que esta versi√≥n es la m√°s robusta y reproducible del proceso.

---

## Extras:

---

```

# Diagrama del proyecto:
Proyecto-Nombres-Empresas/mi_proyecto_escrapeo
‚îú‚îÄ‚îÄ image/
‚îÇ   ‚îî‚îÄ‚îÄ scrapeo/
‚îú‚îÄ‚îÄ mi_proyecto_escrapeo/
‚îÇ   ‚îú‚îÄ‚îÄ escrapeo/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data/                                                    # Archivos CSV de entrada y salida
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ processed/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ contacto_empresas_es.csv
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ empresas_completas.csv
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ raw/
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ comparados.csv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ src/                                                      # Scripts del flujo t√©cnico
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ comparativa.ipynb
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ escrapeo_1.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt                                          # Librer√≠as necesarias para reproducir el entorno para escrapeo
‚îÇ   ‚îî‚îÄ‚îÄ proyecto_correccion/
‚îÇ       ‚îú‚îÄ‚îÄ __pycache__/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ correccion.cpython-312.pyc
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ normalizacion.cpython-312.pyc
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ validacion.cpython-312.pyc
‚îÇ       ‚îú‚îÄ‚îÄ data/                                                      # Archivos CSV de entrada y salida
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ processed/
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ empresas_limpias_corregidas_final.csv
‚îÇ       ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ empresas_limpias_corregidas_mejorado.csv
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ raw/
‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ 100empresas.csv
‚îÇ       ‚îú‚îÄ‚îÄ logs/                                                       # Logs funcionales de la correci√≥n de los nombres de empresas
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ correcciones_sospechosas.csv
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ log_de_correcciones.csv
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ revision_manual.csv
‚îÇ       ‚îú‚îÄ‚îÄ src/                                                        # Scripts del flujo t√©cnico
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ modules/
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ correccion.py
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ normalizacion.py
‚îÇ       ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ validacion.py
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ main.py
‚îÇ       ‚îî‚îÄ‚îÄ requirements.txt                                            # Librer√≠as necesarias para reproducir el entorno para correcci√≥n
‚îú‚îÄ‚îÄ README.md                                                           # Informe t√©cnico en formato Markdown


```

---

```
## Diagrama de flujo funcional del proyecto

Inicio
  ‚Üì
üìÅ Carga de datos iniciales (100empresas.csv)
  ‚Üì
üßπ Correcci√≥n fon√©tica y sem√°ntica
  ‚îú‚îÄ> correccion.py
  ‚îú‚îÄ> normalizacion.py
  ‚îî‚îÄ> validacion.py
  ‚Üì
üìä Generaci√≥n de empresas_limpias_corregidas_final.csv
  ‚Üì
üìù Registro de correcciones en logs/*.csv
  ‚Üì
üåê Scrapeo de nombres corregidos
  ‚îú‚îÄ> escrapeo_1.py
  ‚îî‚îÄ> comparativa.ipynb
  ‚Üì
üìÅ Almacenamiento en contacto_empresas_es.csv y empresas_completas.csv
  ‚Üì
üìö Documentaci√≥n en README.md
  ‚Üì
Fin

```
---



```
## Diagrama de la carpeta "crawler" (escrapeo h√≠brido, asincr√≥nico, con fallback din√°mico y validadci√≥n sem√°ntica, realizado con Crawl4ai).

crawler/
‚îú‚îÄ‚îÄ __pycache__/                     # Archivos compilados autom√°ticamente por Python
‚îÇ   ‚îú‚îÄ‚îÄ contact_extractor.cpython-313.pyc
‚îÇ   ‚îú‚îÄ‚îÄ correccion_crawler.cpython-313.pyc
‚îÇ   ‚îú‚îÄ‚îÄ diccionario_manual.cpython-313.pyc
‚îÇ   ‚îú‚îÄ‚îÄ gestor_correcciones.cpython-313.pyc
‚îÇ   ‚îú‚îÄ‚îÄ normalizacion_crawler.cpython-313.pyc
‚îÇ   ‚îú‚îÄ‚îÄ scraping_utils.cpython-313.pyc
‚îÇ   ‚îî‚îÄ‚îÄ validacion_crawler.cpython-313.pyc

‚îú‚îÄ‚îÄ data/                            # Ecosistema de datos
‚îÇ   ‚îú‚îÄ‚îÄ raw/                         # Datos originales sin procesar
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 100empresas.csv
‚îÇ   ‚îú‚îÄ‚îÄ processed/                   # Datos corregidos, validados y listos para an√°lisis
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ contacto_empresas_es_2.csv*
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ correcciones_sospechosas.csv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ diccionario_nombres_corregidos.csv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ empresas_limpias_corregidas_final.csv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ empresas_limpias_corregidas_mejorado.csv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ nombres_normalizados_para_scraping.csv
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ revision_manual.csv
‚îÇ   ‚îî‚îÄ‚îÄ diccionario_manual.py        # Correcciones manuales y mapeos heur√≠sticos

‚îú‚îÄ‚îÄ logs/                            # Auditor√≠a del proceso y scripts auxiliares
‚îÇ   ‚îú‚îÄ‚îÄ auditoria_texto_extraido.py
‚îÇ   ‚îú‚îÄ‚îÄ contact_extractor.py
‚îÇ   ‚îú‚îÄ‚îÄ gestor_correcciones.py
‚îÇ   ‚îú‚îÄ‚îÄ log_de_correcciones.csv
‚îÇ   ‚îú‚îÄ‚îÄ main_async.py
‚îÇ   ‚îî‚îÄ‚îÄ scraping_utils.py

‚îú‚îÄ‚îÄ src/                             # C√≥digo principal y m√≥dulos reutilizables
‚îÇ   ‚îú‚îÄ‚îÄ modules/                     # Funciones compartidas y componentes t√©cnicos
‚îÇ   ‚îî‚îÄ‚îÄ main.py                      # Script principal de ejecuci√≥n

‚îú‚îÄ‚îÄ compar2.ipynb                    # Notebook de comparaci√≥n o an√°lisis exploratorio
‚îú‚îÄ‚îÄ escrapeo_dos.py                  # Script de scraping h√≠brido con fallback din√°mico
‚îî‚îÄ‚îÄ requirements.txt                 # Lista de dependencias del proyecto

Resultado: extrae los urls con pocos errores, pero direcci√≥n, tel√©fono y email no los alcanza a extraer. En 36 segundos.
```


```
Agregu√© otra versi√≥n mejorada del sistema as√≠ncrona de scraping de informaci√≥n de empresas espa√±olas: version_02
Versi√≥n_02/
‚îú‚îÄ‚îÄ nombres_scraping/                  # Punto de partida del proceso
‚îÇ   ‚îî‚îÄ‚îÄ buscar_url.py                  # Sistema asincr√≥nico que busca URLs
‚îÇ   ‚îî‚îÄ‚îÄ urls.csv                       # Resultado exitoso (¬°por fin!)
‚îÇ
‚îú‚îÄ‚îÄ m√≥dulos_contacto/                 # Extracci√≥n y validaci√≥n de contacto
‚îÇ   ‚îî‚îÄ‚îÄ extractor.py                  # Auxiliar para extracci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ pipeline_contacto.py          # Pipeline principal
‚îÇ   ‚îî‚îÄ‚îÄ contacto.csv                  # Resultado con datos de contacto
‚îÇ
‚îú‚îÄ‚îÄ an√°lisis/                         # Estad√≠sticas y validaciones
‚îÇ   ‚îî‚îÄ‚îÄ estad√≠stica.ipynb             # Notebook con an√°lisis de urls y contacto
‚îú‚îÄ‚îÄ requirements.txt                  # Requerimientos del entorno virtual para trabajarlo
```
